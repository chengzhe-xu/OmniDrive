diff --git a/cpp/CMakeLists.txt b/cpp/CMakeLists.txt
index c275be09..906b1662 100644
--- a/cpp/CMakeLists.txt
+++ b/cpp/CMakeLists.txt
@@ -282,7 +282,7 @@ endif()
 set(CUBLAS_LIB CUDA::cublas)
 set(CUBLASLT_LIB CUDA::cublasLt)
 set(CUDA_DRV_LIB CUDA::cuda_driver)
-set(CUDA_NVML_LIB CUDA::nvml)
+# set(CUDA_NVML_LIB CUDA::nvml)
 set(CUDA_RT_LIB CUDA::cudart_static)
 set(CMAKE_CUDA_RUNTIME_LIBRARY Static)
 
@@ -331,7 +331,7 @@ if(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL "11")
 endif()
 
 if(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL "11.8")
-  add_definitions("-DENABLE_FP8")
+  # add_definitions("-DENABLE_FP8")
   message(
     STATUS
       "CUDAToolkit_VERSION ${CUDAToolkit_VERSION_MAJOR}.${CUDAToolkit_VERSION_MINOR} is greater or equal than 11.8, enable -DENABLE_FP8 flag"
diff --git a/cpp/include/tensorrt_llm/common/mpiUtils.h b/cpp/include/tensorrt_llm/common/mpiUtils.h
index 5583df24..19db030e 100644
--- a/cpp/include/tensorrt_llm/common/mpiUtils.h
+++ b/cpp/include/tensorrt_llm/common/mpiUtils.h
@@ -30,28 +30,28 @@
 #include <cstdlib>
 #include <memory>
 
-#if ENABLE_MULTI_DEVICE
+// #if ENABLE_MULTI_DEVICE
 #include <mpi.h>
-#else
+// #else
 // Dummy defines to avoid #if in wider places.
-typedef int MPI_Datatype;
-typedef int MPI_Comm;
-typedef int MPI_Request;
-typedef int MPI_Message;
-typedef int MPI_Op;
-
-typedef struct MPI_Status
-{
-    int dummy;
-} MPI_Status;
-
-#define MPI_THREAD_SINGLE 0
-#define MPI_THREAD_FUNNELED 1
-#define MPI_THREAD_SERIALIZED 2
-#define MPI_THREAD_MULTIPLE 3
-#define MPI_COMM_WORLD ((MPI_Comm) 0x44000000)
-#define MPI_COMM_NULL ((MPI_Comm) 0x04000000)
-#endif // ENABLE_MULTI_DEVICE
+// typedef int MPI_Datatype;
+// typedef int MPI_Comm;
+// typedef int MPI_Request;
+// typedef int MPI_Message;
+// typedef int MPI_Op;
+
+// typedef struct MPI_Status
+// {
+//     int dummy;
+// } MPI_Status;
+
+// #define MPI_THREAD_SINGLE 0
+// #define MPI_THREAD_FUNNELED 1
+// #define MPI_THREAD_SERIALIZED 2
+// #define MPI_THREAD_MULTIPLE 3
+// #define MPI_COMM_WORLD ((MPI_Comm) 0x44000000)
+// #define MPI_COMM_NULL ((MPI_Comm) 0x04000000)
+// #endif // ENABLE_MULTI_DEVICE
 
 #include <type_traits>
 #include <vector>
diff --git a/cpp/tensorrt_llm/CMakeLists.txt b/cpp/tensorrt_llm/CMakeLists.txt
index a8b6f276..b6769558 100644
--- a/cpp/tensorrt_llm/CMakeLists.txt
+++ b/cpp/tensorrt_llm/CMakeLists.txt
@@ -22,12 +22,10 @@ set(API_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/include)
 include_directories(${CMAKE_CURRENT_SOURCE_DIR}/cutlass_extensions/include
                     ${API_INCLUDE_DIR})
 
-if(ENABLE_MULTI_DEVICE)
-  find_package(MPI REQUIRED)
-  message(STATUS "Using MPI_C_INCLUDE_DIRS: ${MPI_C_INCLUDE_DIRS}")
-  message(STATUS "Using MPI_C_LIBRARIES: ${MPI_C_LIBRARIES}")
-  include_directories(${MPI_C_INCLUDE_DIRS})
-endif()
+find_package(MPI REQUIRED)
+message(STATUS "Using MPI_C_INCLUDE_DIRS: ${MPI_C_INCLUDE_DIRS}")
+message(STATUS "Using MPI_C_LIBRARIES: ${MPI_C_LIBRARIES}")
+include_directories(${MPI_C_INCLUDE_DIRS})
 
 if(NOT WIN32)
   set(DECODER_SHARED_TARGET decoder_attention)
@@ -259,7 +257,7 @@ set(TRTLLM_LINK_LIBS
     kernels_src
     context_attention_src
     decoder_attention_src
-    selective_scan_src
+    # selective_scan_src
     fpA_intB_gemm_src
     moe_gemm_src
     fb_gemm_src
@@ -270,8 +268,9 @@ set(TRTLLM_LINK_LIBS
     ${DECODER_SHARED_TARGET})
 
 if(ENABLE_MULTI_DEVICE)
-  set(TRTLLM_LINK_LIBS ${TRTLLM_LINK_LIBS} ${MPI_C_LIBRARIES} ${NCCL_LIB})
+  set(TRTLLM_LINK_LIBS ${TRTLLM_LINK_LIBS} ${NCCL_LIB})
 endif()
+set(TRTLLM_LINK_LIBS ${TRTLLM_LINK_LIBS} ${MPI_C_LIBRARIES})
 
 if(NOT WIN32) # Unix-like compilers
   set(UNDEFINED_FLAG "-Wl,--no-undefined")
diff --git a/cpp/tensorrt_llm/common/cublasMMWrapper.cpp b/cpp/tensorrt_llm/common/cublasMMWrapper.cpp
index 27f17923..a44afcfa 100644
--- a/cpp/tensorrt_llm/common/cublasMMWrapper.cpp
+++ b/cpp/tensorrt_llm/common/cublasMMWrapper.cpp
@@ -118,7 +118,10 @@ void CublasMMWrapper::Gemm(cublasOperation_t transa, cublasOperation_t transb, i
 void CublasMMWrapper::Gemm(cublasOperation_t transa, cublasOperation_t transb, int const m, int const n, int const k,
     void const* A, int const lda, void const* B, int const ldb, void* C, int const ldc, float f_alpha, float f_beta)
 {
-    bool usingCublasLt = mAType == CUDA_R_16F || mAType == CUDA_R_8F_E4M3;
+    bool usingCublasLt = mAType == CUDA_R_16F;
+#ifdef ENABLE_FP8
+    usingCublasLt = usingCublasLt || mAType == CUDA_R_8F_E4M3;
+#endif
 
     Gemm(transa, transb, m, n, k, A, lda, B, ldb, C, ldc, f_alpha, f_beta, {}, /* hasAlgo */ false,
         /* usingCublasLt */ usingCublasLt);
@@ -132,7 +135,12 @@ void CublasMMWrapper::Gemm(cublasOperation_t transa, cublasOperation_t transb, i
     half h_beta = (half) (f_beta);
 
     // TODO: default cublas libs
+#ifdef ENABLE_FP8
     usingCublasLt = usingCublasLt && (mAType == CUDA_R_16F || mAType == CUDA_R_8F_E4M3);
+#else
+    usingCublasLt = usingCublasLt && mAType == CUDA_R_16F;    
+#endif
+
     bool isFp16ComputeType = mComputeType == CUBLAS_COMPUTE_16F;
     int batch_count = 1;
     // fp32 use cublas as default
diff --git a/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp b/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp
index abe8d0a4..8499249b 100644
--- a/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp
+++ b/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp
@@ -84,7 +84,9 @@ CUDADriverWrapper::CUDADriverWrapper()
     *(void**) (&_cuLinkAddData) = load_sym(handle, "cuLinkAddData_v2");
     *(void**) (&_cuLaunchCooperativeKernel) = load_sym(handle, "cuLaunchCooperativeKernel");
     *(void**) (&_cuLaunchKernel) = load_sym(handle, "cuLaunchKernel");
+#if (__CUDACC_VER_MAJOR__ >= 12)
     *(void**) (&_cuTensorMapEncodeTiled) = load_sym(handle, "cuTensorMapEncodeTiled");
+#endif
     *(void**) (&_cuMemcpyDtoH) = load_sym(handle, "cuMemcpyDtoH_v2");
 }
 
@@ -167,6 +169,7 @@ CUresult CUDADriverWrapper::cuLaunchKernel(CUfunction f, unsigned int gridDimX,
         f, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, kernelParams, extra);
 }
 
+#if (__CUDACC_VER_MAJOR__ >= 12)
 CUresult CUDADriverWrapper::cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType,
     cuuint32_t tensorRank, void* globalAddress, cuuint64_t const* globalDim, cuuint64_t const* globalStrides,
     cuuint32_t const* boxDim, cuuint32_t const* elementStrides, CUtensorMapInterleave interleave,
@@ -175,6 +178,7 @@ CUresult CUDADriverWrapper::cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUten
     return (*_cuTensorMapEncodeTiled)(tensorMap, tensorDataType, tensorRank, globalAddress, globalDim, globalStrides,
         boxDim, elementStrides, interleave, swizzle, l2Promotion, oobFill);
 }
+#endif
 
 CUresult CUDADriverWrapper::cuMemcpyDtoH(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount) const
 {
diff --git a/cpp/tensorrt_llm/common/cudaDriverWrapper.h b/cpp/tensorrt_llm/common/cudaDriverWrapper.h
index a29c3452..c93de4a5 100644
--- a/cpp/tensorrt_llm/common/cudaDriverWrapper.h
+++ b/cpp/tensorrt_llm/common/cudaDriverWrapper.h
@@ -74,12 +74,12 @@ public:
     CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ,
         unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes,
         CUstream hStream, void** kernelParams, void** extra) const;
-
+#if (__CUDACC_VER_MAJOR__ >= 12)
     CUresult cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType, cuuint32_t tensorRank,
         void* globalAddress, cuuint64_t const* globalDim, cuuint64_t const* globalStrides, cuuint32_t const* boxDim,
         cuuint32_t const* elementStrides, CUtensorMapInterleave interleave, CUtensorMapSwizzle swizzle,
         CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill) const;
-
+#endif
     CUresult cuMemcpyDtoH(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount) const;
 
 private:
@@ -101,10 +101,12 @@ private:
     CUresult (*_cuLaunchKernel)(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ,
         unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes,
         CUstream hStream, void** kernelParams, void** extra);
+#if (__CUDACC_VER_MAJOR__ >= 12)
     CUresult (*_cuTensorMapEncodeTiled)(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType,
         cuuint32_t tensorRank, void* globalAddress, cuuint64_t const* globalDim, cuuint64_t const* globalStrides,
         cuuint32_t const* boxDim, cuuint32_t const* elementStrides, CUtensorMapInterleave interleave,
         CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill);
+#endif
     CUresult (*_cuMemcpyDtoH)(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount);
 };
 
diff --git a/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh b/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh
index 1bd0a3f1..e5a21066 100644
--- a/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh
+++ b/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh
@@ -167,12 +167,21 @@ struct Fused_Moe_Kernel_sm80
     }
 };
 
+#if CUDA_VERSION >= 11070
 template <typename GemmType>
 __global__ void run_global(__grid_constant__ typename GemmType::Params const params)
 {
     GemmType gemm;
     gemm.run_device(params);
 }
+#else
+template <typename GemmType>
+__global__ void run_global(typename GemmType::Params const params)
+{
+    GemmType gemm;
+    gemm.run_device(params);
+}
+#endif
 
 /// Computes the maximum number of active blocks per multiprocessor
 template <typename GemmType>
diff --git a/cpp/tensorrt_llm/kernels/CMakeLists.txt b/cpp/tensorrt_llm/kernels/CMakeLists.txt
index 6d908153..4e2985f8 100644
--- a/cpp/tensorrt_llm/kernels/CMakeLists.txt
+++ b/cpp/tensorrt_llm/kernels/CMakeLists.txt
@@ -67,4 +67,4 @@ set_property(TARGET kernels_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)
 add_subdirectory(cutlass_kernels)
 add_subdirectory(contextFusedMultiHeadAttention)
 add_subdirectory(decoderMaskedMultiheadAttention)
-add_subdirectory(selectiveScan)
+# add_subdirectory(selectiveScan)
diff --git a/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp b/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp
index 5857e927..657a26bb 100644
--- a/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp
+++ b/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp
@@ -80,7 +80,7 @@ static inline void set_alpha(uint32_t& alpha, float norm, Data_type dtype)
 FusedMHARunnerV2::FusedMHARunnerV2(MHARunnerFixedParams fixedParams)
     : mFixedParams(fixedParams)
 {
-    TLLM_CHECK_WITH_INFO((mSM == kSM_70 || mSM == kSM_80 || mSM == kSM_86 || mSM == kSM_89 || mSM == kSM_90),
+    TLLM_CHECK_WITH_INFO((mSM == kSM_70 || mSM == kSM_80 || mSM == kSM_86 || mSM == kSM_87 || mSM == kSM_89 || mSM == kSM_90),
         "Unsupported architecture");
     TLLM_CHECK_WITH_INFO((mFixedParams.dataType == DATA_TYPE_FP16 || mFixedParams.dataType == DATA_TYPE_BF16
                              || mFixedParams.dataType == DATA_TYPE_E4M3),
diff --git a/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu b/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu
index 5b230443..b07e0a67 100644
--- a/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu
+++ b/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu
@@ -339,6 +339,7 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
         smem_size = params.fusion_params.hidden_size * sizeof(T);
         if (tensorrt_llm::common::getEnvEnableFDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable FDL in rms_norm_kernel");
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = cta_num;
@@ -354,6 +355,9 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
 
             TLLM_CUDA_CHECK(
                 cudaLaunchKernelEx(&kernelConfig, rms_norm_kernel<T, Bias, Residual, Affine, true>, params));
+            #else
+                TLLM_THROW("unsupported");
+            #endif            
         }
         else
         {
@@ -364,6 +368,7 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
     {
         if (tensorrt_llm::common::getEnvEnableFDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable FDL in rms_norm_kernel");
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = cta_num;
@@ -379,6 +384,9 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
 
             TLLM_CUDA_CHECK(
                 cudaLaunchKernelEx(&kernelConfig, rms_norm_kernel<T, Bias, Residual, Affine, false>, params));
+            #else
+            TLLM_THROW("unsupported");
+            #endif            
         }
         else
         {
@@ -519,6 +527,7 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
         smem_size = params.fusion_params.hidden_size * sizeof(T);
         if (tensorrt_llm::common::getEnvEnableFDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable FDL in one_shot_all_reduce_norm_kernel");
 
             cudaLaunchConfig_t kernelConfig = {0};
@@ -535,6 +544,9 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
 
             TLLM_CUDA_CHECK(cudaLaunchKernelEx(
                 &kernelConfig, one_shot_all_reduce_norm_kernel<T, RanksPerNode, Bias, Affine, true>, params));
+            #else
+            TLLM_THROW("unsupported");
+            #endif            
         }
         else
         {
@@ -546,6 +558,7 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
     {
         if (tensorrt_llm::common::getEnvEnableFDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = cta_num;
             kernelConfig.blockDim = cta_size;
@@ -561,6 +574,9 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
             TLLM_LOG_DEBUG("Enable FDL in one_shot_all_reduce_norm_kernel");
             TLLM_CUDA_CHECK(cudaLaunchKernelEx(
                 &kernelConfig, one_shot_all_reduce_norm_kernel<T, RanksPerNode, Bias, Affine, false>, params));
+            #else
+            TLLM_THROW("unsupported");
+            #endif            
         }
         else
         {
@@ -982,6 +998,7 @@ void AllReduceNormKernelLaunch(AllReduceStrategyType algo, AllReduceStrategyConf
 
         if (tensorrt_llm::common::getEnvEnableFDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable FDL in twoShotAllReduceKernel");
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = blocks_per_grid;
@@ -997,6 +1014,9 @@ void AllReduceNormKernelLaunch(AllReduceStrategyType algo, AllReduceStrategyConf
 
             TLLM_CUDA_CHECK(cudaLaunchKernelEx(
                 &kernelConfig, twoShotAllReduceKernel<T, RANKS_PER_NODE, !USE_MEMCPY, PUSH_MODE, Bias, true>, params));
+            #else
+            TLLM_THROW("unsupported");
+            #endif
         }
         else
         {
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h
index 0ec8ab2e..501ff652 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h
@@ -18,7 +18,9 @@
 
 #include <cuda_bf16.h>
 #include <cuda_fp16.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 
 #include "cutlass/bfloat16.h"
 #include "cutlass/float8.h"
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h
index f1b7bb0f..c0c1c2d5 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h
@@ -256,6 +256,7 @@ void filter_and_run_mixed_gemm(ActivationType const* A, WeightType const* B, Sca
             + std::to_string(arch::kMinComputeCapability) + " with stages set to " + std::to_string(Stages);
         throw std::runtime_error("[TensorRT-LLm Error][filter_and_run_mixed_gemm] " + err_msg);
     }
+#if defined(ENABLE_FP8)
     else if constexpr (cutlass::platform::is_same<ActivationType, __nv_fp8_e4m3>::value
         && arch::kMinComputeCapability < 89)
     {
@@ -264,6 +265,7 @@ void filter_and_run_mixed_gemm(ActivationType const* A, WeightType const* B, Sca
             + std::to_string(arch::kMinComputeCapability) + " with activation type set to FP8";
         throw std::runtime_error("[TensorRT-LLm Error][filter_and_run_mixed_gemm] " + err_msg);
     }
+#endif
     else
     {
         generic_mixed_gemm_kernelLauncher<ActivationType, WeightType, ScaleZeroType, BiasType, OutputType, arch,
@@ -309,7 +311,11 @@ void dispatch_gemm_config(ActivationType const* A, WeightType const* B, ScaleZer
 template <typename T>
 constexpr bool is_fp8()
 {
+#if defined(ENABLE_FP8)
     return std::is_same_v<T, __nv_fp8_e4m3> || std::is_same_v<T, __nv_fp8_e5m2>;
+#else
+    return false;
+#endif
 }
 
 template <typename ActivationType, typename WeightType, typename ScaleZeroType, typename BiasType, typename OutputType,
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h
index dd7bd96c..e499e293 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h
@@ -56,10 +56,10 @@ void sm90_dispatch_epilogue_schedules(ActivationType const* A, WeightType const*
     case tkc::EpilogueScheduleType::AUTO:
         using EpilogueScheduleType = cute::conditional_t<size<0>(CTAShape{}) == Int<64>{},
             cutlass::epilogue::TmaWarpSpecialized, cutlass::epilogue::TmaWarpSpecializedCooperative>;
-        sm90_generic_mixed_gemm_kernelLauncher<ActivationType, WeightType, ScaleZeroType, BiasType, OutputType, QuantOp,
-            EpilogueTag, CTAShape, ClusterShape, MainloopScheduleType, EpilogueScheduleType>(A, B, weight_scales,
-            weight_zero_points, biases, alpha, C, m, n, k, group_size, gemm_config, workspace, workspace_bytes, stream,
-            occupancy);
+        // sm90_generic_mixed_gemm_kernelLauncher<ActivationType, WeightType, ScaleZeroType, BiasType, OutputType, QuantOp,
+        //     EpilogueTag, CTAShape, ClusterShape, MainloopScheduleType, EpilogueScheduleType>(A, B, weight_scales,
+        //     weight_zero_points, biases, alpha, C, m, n, k, group_size, gemm_config, workspace, workspace_bytes, stream,
+        //     occupancy);
         break;
     default:
         throw std::runtime_error(
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h
index 8ef0f9bb..089a77a1 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h
@@ -217,6 +217,7 @@ size_t dispatchGemmToCutlassSm90(void* D, void const* A, void const* B, void con
     char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr)
 {
     TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
+#if defined(ENABLE_FP8)
     static_assert(std::is_same_v<T, __nv_fp8_e4m3>, "fusedGatedGemmSm90 only support FP8(e4m3)");
     constexpr int Ktile = 128 / sizeof(T);
     using _Ktile = Int<Ktile>;
@@ -270,6 +271,7 @@ size_t dispatchGemmToCutlassSm90(void* D, void const* A, void const* B, void con
             "gated GEMM.");
         break;
     }
+#endif
 }
 
 template <typename T>
@@ -290,6 +292,7 @@ size_t CutlassFusedGatedGemmRunner<T>::dispatchToArch(void* D, void const* A, vo
     tk::QuantMode quantOption, int m, int n, int k, float scale_d0, float scale_d1, float scale_output,
     tkc::CutlassGemmConfig gemmConfig, char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy)
 {
+#if defined(ENABLE_FP8)
     TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
     if constexpr (std::is_same_v<T, __nv_fp8_e4m3>)
     {
@@ -312,6 +315,7 @@ size_t CutlassFusedGatedGemmRunner<T>::dispatchToArch(void* D, void const* A, vo
             "gated "
             "GEMM");
     }
+#endif
     return 0;
 }
 
@@ -334,6 +338,7 @@ std::vector<tkc::CutlassGemmConfig> CutlassFusedGatedGemmRunner<T>::getConfigs()
 
     std::vector<CutlassGemmConfig> candidateConfigs;
 
+#if defined(ENABLE_FP8)
     if constexpr (std::is_same_v<T, __nv_fp8_e4m3>)
     {
         if (mSm != 90)
@@ -381,6 +386,7 @@ std::vector<tkc::CutlassGemmConfig> CutlassFusedGatedGemmRunner<T>::getConfigs()
             "gated "
             "GEMM");
     }
+#endif
     return candidateConfigs;
 }
 
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu
index 2e603cfb..813a1851 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu
@@ -22,7 +22,9 @@ namespace kernels
 {
 namespace cutlass_kernels
 {
+#if defined(ENABLE_FP8)
 template class CutlassFusedGatedGemmRunner<__nv_fp8_e4m3>;
+#endif
 } // namespace cutlass_kernels
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h
index 0616c063..fcb2517e 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h
@@ -61,7 +61,7 @@ struct HopperGroupedGemmInput
     using StrideC = std::remove_pointer_t<cutlass::detail::TagToStrideC_t<LayoutC*>>;
 
     template <class T>
-    constexpr static bool IsFP8_v = std::is_same_v<T, __nv_fp8_e4m3> || std::is_same_v<T, __nv_fp8_e5m2>;
+    constexpr static bool IsFP8_v = false; // = std::is_same_v<T, __nv_fp8_e4m3> || std::is_same_v<T, __nv_fp8_e5m2>;
 
     // Currently this should always just be T
     template <class T>
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py b/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py
index 096a8fe2..2433fe65 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py
@@ -83,7 +83,7 @@ QuantOpTag = {
 # The activations, biases, scales and zeros are instantiated using CUDA types,
 # not CUTLASS types. This map materializes the name of the CUDA type.
 CudaTypeName = {
-    DataType.e4m3: "__nv_fp8_e4m3",
+    # DataType.e4m3: "__nv_fp8_e4m3",
     DataType.bf16: "__nv_bfloat16",
     DataType.f16: "half",
     DataType.f32: "float"
@@ -229,8 +229,8 @@ def instantiate_operation_sm80(operation):
 def instantiate_operation(operation):
     if operation.arch == 80:
         return instantiate_operation_sm80(operation)
-    elif operation.arch == 90:
-        return instantiate_operation_sm90(operation)
+    # elif operation.arch == 90:
+    #     return instantiate_operation_sm90(operation)
 
 
 def get_file_content(launcher_inl_files, operations):
@@ -320,7 +320,7 @@ def generate_sm90_mixed_gemm_operations():
     # will remap those back to the signed type.
     # Takes the form (activation_type, weight_type, scalezero_type, bias_type, output_type)
     supported_dtypes = [
-        (DataType.e4m3, DataType.u4, DataType.f16, DataType.f16, DataType.f16),
+        # (DataType.e4m3, DataType.u4, DataType.f16, DataType.f16, DataType.f16),
         (DataType.f16, DataType.u4, DataType.f16, DataType.f16, DataType.f16),
         (DataType.bf16, DataType.u4, DataType.bf16, DataType.bf16,
          DataType.bf16),
@@ -487,8 +487,8 @@ if __name__ == "__main__":
 
     # The goal here is to group kernels with common instantiations together in order to reduce template instantiation overheads.
     # Template instantiation dominates the time in a compilation unit, so it is the most important factor to improve.
-    operations = generate_sm90_operations()
-    operations += generate_sm80_operations()
+    # operations = generate_sm90_operations()
+    operations = generate_sm80_operations()
     op_groups = dict()
     for op in operations:
         dict_key = (op.gemm_kind, op.arch, op.cta_shape[0])
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h
index ab88bc20..027189be 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h
@@ -183,26 +183,7 @@ inline void multi_block_grid_setup(dim3& grid, Multihead_attention_params<T, DO_
     const auto mmhaFunc = mmha::masked_multihead_attention_kernel<T, T_cache, TKcache, KVCacheBuffer, KCacheBuffer,    \
         Dh, DYNAMIC_THDS_PER_BLOCK, KernelParamsType::DO_CROSS_ATTENTION, HAS_BEAMS, ENABLE_MULTI_BLOCK, POS_SHIFT,    \
         BLOCK_SPARSE_ATTN, IMPLICIT_REL_ATTN_BIAS, QK_TANH_SCALE>;                                                     \
-    if (tensorrt_llm::common::getEnvEnableFDL())                                                                       \
-    {                                                                                                                  \
-        TLLM_LOG_DEBUG("Enable FDL in MMHA");                                                                          \
-        cudaLaunchConfig_t kernelConfig = {0};                                                                         \
-        kernelConfig.gridDim = grid;                                                                                   \
-        kernelConfig.blockDim = DYNAMIC_THDS_PER_BLOCK;                                                                \
-        kernelConfig.dynamicSmemBytes = dynamic_smem_sz;                                                               \
-        kernelConfig.stream = stream;                                                                                  \
-                                                                                                                       \
-        cudaLaunchAttribute attribute[1];                                                                              \
-        attribute[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;                                          \
-        attribute[0].val.programmaticStreamSerializationAllowed = 1;                                                   \
-        kernelConfig.attrs = attribute;                                                                                \
-        kernelConfig.numAttrs = 1;                                                                                     \
-        TLLM_CUDA_CHECK(cudaLaunchKernelEx(&kernelConfig, mmhaFunc, params, kv_cache_buffer, k_cache_buffer));         \
-    }                                                                                                                  \
-    else                                                                                                               \
-    {                                                                                                                  \
-        mmhaFunc<<<grid, DYNAMIC_THDS_PER_BLOCK, dynamic_smem_sz, stream>>>(params, kv_cache_buffer, k_cache_buffer);  \
-    }
+    mmhaFunc<<<grid, DYNAMIC_THDS_PER_BLOCK, dynamic_smem_sz, stream>>>(params, kv_cache_buffer, k_cache_buffer);
 
 // if resources are not enough to launch 512 threads per block, we will fallback to 256.
 #define MMHA_512_BLOCKSIZE_CHECK()                                                                                     \
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h
index 6671b46f..c2113e49 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h
@@ -1210,6 +1210,7 @@ struct Launch_bounds_config
     static constexpr int MIN_BLOCKS_PER_SM = 0;
 };
 
+#ifdef ENABLE_FP8
 template <>
 struct Launch_bounds_config<uint16_t, __nv_fp8_e4m3, 256u, 64u, false, false, false>
 {
@@ -1232,7 +1233,7 @@ struct Launch_bounds_config<uint16_t, __nv_fp8_e4m3, 256u, 256u, false, true, fa
     static constexpr int MAX_THREADS_PER_BLOCK = 256u;
     static constexpr int MIN_BLOCKS_PER_SM = 3u;
 };
-
+#endif
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 inline __device__ constexpr uint32_t shfl_mask(int threads)
@@ -1312,8 +1313,8 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
     static constexpr bool ENABLE_8BITS_K_CACHE = sizeof(TKcache) == 1;
     static constexpr bool ENABLE_8BITS_KV_CACHE = sizeof(Tcache) == 1;
     // FP8 KV Cache.
-    static constexpr bool FP8_K_CACHE = std::is_same<TKcache, __nv_fp8_e4m3>::value;
-    static constexpr bool FP8_KV_CACHE = std::is_same<Tcache, __nv_fp8_e4m3>::value;
+    static constexpr bool FP8_K_CACHE = false; // std::is_same<TKcache, __nv_fp8_e4m3>::value;
+    static constexpr bool FP8_KV_CACHE = false; // = std::is_same<Tcache, __nv_fp8_e4m3>::value;
     // INT8 KV Cache.
     static constexpr bool INT8_KV_CACHE = std::is_same<Tcache, int8_t>::value;
 
@@ -2644,8 +2645,8 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
     }
 
     // Quantized output only supports fp8 currently, which should be used together with FP8 Context FMHA.
-    using Quantized_t = __nv_fp8_e4m3;
-    using Quantized_vec = typename packed_type<__nv_fp8_e4m3, num_elems<V_vec_accum>::value>::type;
+    // using Quantized_t = __nv_fp8_e4m3;
+    // using Quantized_vec = typename packed_type<__nv_fp8_e4m3, num_elems<V_vec_accum>::value>::type;
     auto const bhi = tensorrt_llm::common::flat_index2(batch_beam_idx, hi, num_heads);
     auto const bhi_seq_len_tile = bhi * params.seq_len_tile;
     // Output the final values.
@@ -2655,20 +2656,20 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
 #ifdef MMHA_USE_FP32_ACCUM_FOR_OUT
         if (!MULTI_BLOCK_FLAG)
         {
-            if (write_attention_quant)
-            {
-                out = mul<V_vec_accum, float>(*params.attention_out_scale_orig_quant, out);
-                Quantized_vec final_out;
-                convert_to_fp8(&final_out, out);
-                *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhvi) = final_out;
-            }
-            else
-            {
+            // if (write_attention_quant)
+            // {
+            //     out = mul<V_vec_accum, float>(*params.attention_out_scale_orig_quant, out);
+            //     Quantized_vec final_out;
+            //     convert_to_fp8(&final_out, out);
+            //     *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhvi) = final_out;
+            // }
+            // else
+            // {
                 // This makes sure we have coalesced memory access.
                 V_vec_k final_out;
                 convert_from_float(&final_out, out);
                 *reinterpret_cast<V_vec_k*>(static_cast<T*>(params.out) + bhvi) = final_out;
-            }
+            // }
         }
         else
         {
@@ -2834,17 +2835,17 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
 
                 thread_accumulated_out = mul<V_vec_k, Tk, V_vec_k>(inv_sum_compute, thread_accumulated_out);
 
-                if (write_attention_quant)
-                {
-                    Quantized_vec final_out;
-                    convert_to_fp8(&final_out, thread_accumulated_out);
-                    *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhi * Dh + oi)
-                        = final_out;
-                }
-                else
-                {
+                // if (write_attention_quant)
+                // {
+                //     Quantized_vec final_out;
+                //     convert_to_fp8(&final_out, thread_accumulated_out);
+                //     *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhi * Dh + oi)
+                //         = final_out;
+                // }
+                // else
+                // {
                     *reinterpret_cast<V_vec_k*>(static_cast<T*>(params.out) + (bhi * Dh + oi)) = thread_accumulated_out;
-                }
+                // }
             }
 
             // Reset qk_current_smem and block_counter for the next timestep
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp
index 8bfd097e..7ce0d713 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp
@@ -275,12 +275,14 @@ void DecoderXQAImplJIT::runImpl(XQAParams const& xqaParams, KVCacheBuffer const&
     }
     appendParam(&launchParams.batch_size);
     appendParam(&launchParams.kv_scale_quant_orig);
+#if (__CUDACC_VER_MAJOR__ >= 12)
     CUtensorMap tensorMap{};
     if (isGMMAKernel)
     {
         tensorMap = makeTensorMapForKVCache(mDriver, xqaParams, kv_cache_buffer);
         appendParam(&tensorMap);
     }
+#endif
     appendParam(&launchParams.semaphores);
     appendParam(&launchParams.scratch);
     kernelParams[idxNextParam] = nullptr; // one extra nullptr at end as guard.
@@ -295,6 +297,7 @@ void DecoderXQAImplJIT::runImpl(XQAParams const& xqaParams, KVCacheBuffer const&
     cubinObj->launch(gridDim, blockDim, stream, kernelParams);
     sync_check_cuda_error();
 
+#ifdef ENABLE_FP8
     if (needOutputCvt)
     {
         tensorrt_llm::kernels::invokeConversion<__nv_fp8_e4m3, T>(static_cast<__nv_fp8_e4m3*>(xqaParams.output),
@@ -303,6 +306,7 @@ void DecoderXQAImplJIT::runImpl(XQAParams const& xqaParams, KVCacheBuffer const&
             stream);
         sync_check_cuda_error();
     }
+#endif
 }
 
 } // namespace kernels
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp
index f1736651..a113fd04 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp
@@ -284,12 +284,12 @@ public:
             }
             appendParam(&launchParams.batch_size);
             appendParam(&launchParams.kv_scale_quant_orig);
-            CUtensorMap tensorMap{};
-            if (isGmmaKernel)
-            {
-                tensorMap = makeTensorMapForKVCache(mDriver, xqaParams, kv_cache_buffer);
-                appendParam(&tensorMap);
-            }
+            // CUtensorMap tensorMap{};
+            // if (isGmmaKernel)
+            // {
+            //     tensorMap = makeTensorMapForKVCache(mDriver, xqaParams, kv_cache_buffer);
+            //     appendParam(&tensorMap);
+            // }
             appendParam(&launchParams.semaphores);
             appendParam(&launchParams.scratch);
             kernelParams[idxNextParam] = nullptr; // one extra nullptr at end as guard.
@@ -305,14 +305,14 @@ public:
 
         sync_check_cuda_error();
 
-        if (needOutputCvt)
-        {
-            tensorrt_llm::kernels::invokeConversion<__nv_fp8_e4m3, T>(static_cast<__nv_fp8_e4m3*>(xqaParams.output),
-                static_cast<T const*>(launchParams.output),
-                xqaParams.head_size * xqaParams.num_q_heads * xqaParams.total_num_input_tokens, xqaParams.fp8_out_scale,
-                stream);
-            sync_check_cuda_error();
-        }
+        // if (needOutputCvt)
+        // {
+        //     tensorrt_llm::kernels::invokeConversion<__nv_fp8_e4m3, T>(static_cast<__nv_fp8_e4m3*>(xqaParams.output),
+        //         static_cast<T const*>(launchParams.output),
+        //         xqaParams.head_size * xqaParams.num_q_heads * xqaParams.total_num_input_tokens, xqaParams.fp8_out_scale,
+        //         stream);
+        //     sync_check_cuda_error();
+        // }
     }
 
 protected:
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp
index ca2a2c57..2457f39b 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp
@@ -24,6 +24,7 @@ namespace tensorrt_llm
 namespace kernels
 {
 
+#if __CUDACC_VER_MAJOR__ >= 12
 namespace
 {
 
@@ -138,6 +139,6 @@ template CUtensorMap makeTensorMapForKVCache(
     std::shared_ptr<CUDADriverWrapper> const&, XQAParams const&, KVBlockArray const&);
 template CUtensorMap makeTensorMapForKVCache(
     std::shared_ptr<CUDADriverWrapper> const&, XQAParams const&, KVLinearBuffer const&);
-
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h
index 8570c418..4ce9256b 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h
@@ -22,9 +22,11 @@ namespace tensorrt_llm
 namespace kernels
 {
 
+#if __CUDACC_VER_MAJOR__ >= 12
 template <typename KVCacheBuffer>
 CUtensorMap makeTensorMapForKVCache(std::shared_ptr<tensorrt_llm::common::CUDADriverWrapper> const& driver,
     XQAParams const& xqaParams, KVCacheBuffer const& kv_cache_buffer);
-
+#else
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h
index 59ad3dd5..f14a55cf 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h
@@ -399,7 +399,7 @@ inline __device__ float4 add(float4 a, float4 b)
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-#ifdef ENABLE_FP8
+// #ifdef ENABLE_FP8
 inline __device__ Float8_ add(Float8_ a, Float8_ b)
 {
     Float8_ c;
@@ -409,7 +409,7 @@ inline __device__ Float8_ add(Float8_ a, Float8_ b)
     c.w = add(a.w, b.w);
     return c;
 }
-#endif
+// #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
diff --git a/cpp/tensorrt_llm/kernels/lruKernel.cu b/cpp/tensorrt_llm/kernels/lruKernel.cu
deleted file mode 100644
index a0fc4fdb..00000000
--- a/cpp/tensorrt_llm/kernels/lruKernel.cu
+++ /dev/null
@@ -1,440 +0,0 @@
-/*
- * Copyright (c) 2020-2024, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <cuda_runtime_api.h>
-
-#include <cooperative_groups/memcpy_async.h>
-#include <cuda/pipeline>
-
-#include <cuda_bf16.h>
-#include <cuda_fp16.h>
-
-#ifdef ENABLE_FP8
-#include <cuda_fp8.h>
-#endif
-
-#include "lruKernel.h"
-#include "tensorrt_llm/common/cudaTypeUtils.cuh"
-
-using namespace tensorrt_llm::common;
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-
-__forceinline__ __device__ float copysignf_pos(float a, float b)
-{
-    float r;
-    r = __int_as_float(__float_as_int(a) | (__float_as_int(b) & 0x80000000));
-    return r;
-}
-
-#pragma nv_diag_suppress static_var_with_dynamic_init
-
-template <typename T, int CHANNELS_PER_BLOCK = 128, int STAGES = 20, int SEQ_UNROLL = 10>
-__launch_bounds__(256, 1) __global__ void rg_lru_kernel(lruParams params)
-{
-    T* output = reinterpret_cast<T*>(params.out_ptr);
-    float* state = reinterpret_cast<float*>(params.state_ptr);
-    T* x = reinterpret_cast<T*>(params.x_ptr);
-    T* y = reinterpret_cast<T*>(params.y_ptr);
-    T* y_bias = reinterpret_cast<T*>(params.y_bias_ptr);
-    T* A = reinterpret_cast<T*>(params.A_ptr);
-    int num_channels = params.width;
-    int block_size = params.block_size;
-
-    bool enable_fuse_gate = (params.gate_ptr != nullptr);
-    bool enable_gate_bias;
-    T *gate_x, *gate_a, *gate_x_bias, *gate_a_bias;
-    if (enable_fuse_gate)
-    {
-        enable_gate_bias = (params.gate_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-        }
-    }
-    else
-    {
-        enable_gate_bias = (params.gate_x_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_x_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_a_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_x_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_a_bias_ptr);
-        }
-    }
-
-    __shared__ cuda::pipeline_shared_state<cuda::thread_scope::thread_scope_block, STAGES / SEQ_UNROLL> pipeline_state;
-    auto block = cooperative_groups::this_thread_block();
-
-    __shared__ __align__(128) T sh_gx[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_gate_x_bias[CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_ga[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_gate_a_bias[CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_x[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_y[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_y_bias[CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) float sh_a[CHANNELS_PER_BLOCK];
-
-    int const channel = blockIdx.x * blockDim.x + threadIdx.x;
-    int const sample = blockIdx.y; // batch id
-    int const tid = threadIdx.x;
-
-    int const slot_idx = params.slot_mapping_ptr == nullptr ? sample : params.slot_mapping_ptr[sample];
-    int num_tokens;
-    int start_token_idx;
-    if (params.remove_padding)
-    {
-        start_token_idx = sample == 0 ? 0 : params.last_token_ids_ptr[sample - 1];
-        int end_token_idx = params.last_token_ids_ptr[sample];
-        num_tokens = end_token_idx - start_token_idx;
-    }
-    else
-    {
-        start_token_idx = sample * params.max_seqlen;
-        num_tokens = params.last_token_ids_ptr[sample];
-    }
-
-    int const seq_loops = (num_tokens + SEQ_UNROLL - 1) / SEQ_UNROLL;
-    int const block_channel_base = start_token_idx * num_channels + blockIdx.x * blockDim.x;
-    int const gate_num_channels = enable_fuse_gate ? num_channels * 2 : num_channels;
-    int const gate_block_channel_base = start_token_idx * gate_num_channels;
-    int const tid_offset = tid < 64 ? 32 : 64;
-    int const gchannel = sizeof(T) == 4 ? channel : blockIdx.x * blockDim.x + (threadIdx.x - tid_offset) * 4;
-    int const gx_dim_idx = enable_fuse_gate ? gchannel / block_size * block_size * 2 + gchannel % block_size : gchannel;
-    int const ga_dim_idx = enable_fuse_gate ? gx_dim_idx + block_size : gchannel;
-    int const gx_bias_idx = enable_fuse_gate ? channel / block_size * block_size * 2 + channel % block_size : channel;
-    int const ga_bias_idx = enable_fuse_gate ? gx_bias_idx + block_size : channel;
-
-    if (threadIdx.y == 1)
-    {
-        // Data loading warps
-
-        // Bias and param A are independent of token
-        if (y_bias)
-            sh_y_bias[tid] = y_bias[channel];
-        if (enable_gate_bias)
-        {
-            sh_gate_x_bias[tid] = gate_x_bias[gx_bias_idx];
-            sh_gate_a_bias[tid] = gate_a_bias[ga_bias_idx];
-        }
-        float param_a = cuda_cast<float>(A[channel]);
-        sh_a[tid] = param_a <= 20.f ? -8.0f * __logf(1.0f + __expf(param_a)) : -8.0f * param_a;
-
-        cuda::pipeline pipeline = cuda::make_pipeline(block, &pipeline_state, cuda::pipeline_role::producer);
-
-        int stage = 0;
-        for (int si = 0; si < seq_loops; si++)
-        {
-            pipeline.producer_acquire();
-#pragma unroll
-            for (int token_id = si * SEQ_UNROLL; token_id < num_tokens && token_id < (si + 1) * SEQ_UNROLL; token_id++)
-            {
-                int block_channel = block_channel_base + token_id * num_channels;
-                int gate_block_channel = gate_block_channel_base + token_id * gate_num_channels;
-                if (sizeof(T) == 4)
-                {
-                    cuda::memcpy_async(
-                        &sh_gx[stage][tid], &gate_x[gate_block_channel + gx_dim_idx], sizeof(T), pipeline);
-                    cuda::memcpy_async(
-                        &sh_ga[stage][tid], &gate_a[gate_block_channel + ga_dim_idx], sizeof(T), pipeline);
-                    cuda::memcpy_async(&sh_x[stage][tid], &x[block_channel + tid], sizeof(T), pipeline);
-                    if (y)
-                        cuda::memcpy_async(&sh_y[stage][tid], &y[block_channel + tid], sizeof(T), pipeline);
-                }
-                else
-                {
-                    if (tid < 32)
-                    {
-                        float2* block_x = (float2*) &x[block_channel];
-                        cuda::memcpy_async((float2*) &sh_x[stage][tid * 4], &block_x[tid], sizeof(float2), pipeline);
-                    }
-                    else if (tid < 64)
-                    {
-                        int tid_tmp = tid - 32;
-                        float2* block_gx = (float2*) &gate_x[gate_block_channel];
-                        cuda::memcpy_async(
-                            (float2*) &sh_gx[stage][tid_tmp * 4], &block_gx[gx_dim_idx >> 2], sizeof(float2), pipeline);
-                    }
-                    else if (tid < 96)
-                    {
-                        int tid_tmp = tid - 64;
-                        float2* block_ga = (float2*) &gate_a[gate_block_channel];
-                        cuda::memcpy_async(
-                            (float2*) &sh_ga[stage][tid_tmp * 4], &block_ga[ga_dim_idx >> 2], sizeof(float2), pipeline);
-                    }
-                    else if (tid < 128)
-                    {
-                        if (y)
-                        {
-                            int tid_tmp = tid - 96;
-                            float2* block_y = (float2*) &y[block_channel];
-                            cuda::memcpy_async(
-                                (float2*) &sh_y[stage][tid_tmp * 4], &block_y[tid_tmp], sizeof(float2), pipeline);
-                        }
-                    }
-                }
-                stage++;
-                if (stage >= STAGES)
-                    stage = 0;
-            }
-            pipeline.producer_commit();
-        }
-    }
-    else
-    {
-        // Compute warps
-
-        cuda::pipeline pipeline = cuda::make_pipeline(block, &pipeline_state, cuda::pipeline_role::consumer);
-
-        float state_reg = 0.f;
-        int stage = 0;
-
-        for (int si = 0; si < seq_loops; si++)
-        {
-            pipeline.consumer_wait();
-#pragma unroll
-            for (int token_id = si * SEQ_UNROLL; token_id < num_tokens && token_id < (si + 1) * SEQ_UNROLL; token_id++)
-            {
-                // Read y
-                float y_reg;
-                if (y_bias)
-                {
-                    y_reg = cuda_cast<float>(sh_y[stage][tid] + sh_y_bias[tid]);
-                    // GELU
-                    float k0 = float(0.7978845608028654);
-                    float k1 = float(0.044715);
-                    float y_tanh = k0 * y_reg * (1.0 + k1 * y_reg * y_reg);
-                    float exp_val = -1.f * cuda_abs(y_tanh * 2);
-                    y_reg = 0.5f * y_reg
-                        * (1.f + copysignf_pos(__fdividef((1.f - __expf(exp_val)), (1.f + __expf(exp_val))), y_tanh));
-                }
-                else if (y)
-                {
-                    y_reg = cuda_cast<float>(sh_y[stage][tid]);
-                }
-                else
-                {
-                    y_reg = 1.f;
-                }
-                // Read gate_x
-                float gate_x_reg, gate_a_reg;
-                if (enable_gate_bias)
-                {
-                    gate_x_reg = cuda_cast<float>(-sh_gx[stage][tid] - sh_gate_x_bias[tid]);
-                    gate_a_reg = cuda_cast<float>(-sh_ga[stage][tid] - sh_gate_a_bias[tid]);
-                }
-                else
-                {
-                    gate_x_reg = cuda_cast<float>(-sh_gx[stage][tid]);
-                    gate_a_reg = cuda_cast<float>(-sh_ga[stage][tid]);
-                }
-                // Get gated inputs
-                float x_reg = cuda_cast<float>(sh_x[stage][tid]);
-                float sigmoid_x = __fdividef(1.0f, (1.0f + __expf(gate_x_reg)));
-                float sigmoid_a = __fdividef(1.0f, (1.0f + __expf(gate_a_reg)));
-                float log_a = sigmoid_a * sh_a[tid];
-                float a = __expf(log_a);
-                float a_square = __expf(2.0 * log_a);
-                float outf = y_reg;
-                float normalized_x = x_reg * sigmoid_x;
-                if (si != 0 || token_id != 0)
-                    normalized_x *= sqrtf(1 - a_square);
-
-                // RNN scan
-                state_reg = a * state_reg + normalized_x;
-                outf *= state_reg;
-
-                // Write output
-                T* out = &output[start_token_idx * num_channels + token_id * num_channels];
-                out[channel] = cuda_cast<T>(outf);
-
-                stage++;
-                if (stage >= STAGES)
-                    stage = 0;
-            }
-            pipeline.consumer_release();
-        }
-        // Write the new state back out to the cache
-        state[slot_idx * num_channels + channel] = state_reg;
-    }
-}
-
-template <typename T>
-void invokeRGLRU(lruParams& params, cudaStream_t stream)
-{
-    int samples = params.batch;
-    int channels = params.width;
-
-    int const threads = 128;
-    int const blocks = (channels + threads - 1) / threads;
-    dim3 block(threads, 2);
-    dim3 grid(blocks, samples);
-    TLLM_CHECK((channels % block.x) == 0);
-    TLLM_CHECK(!(params.block_size % 4 != 0 && sizeof(T) == 2));
-
-    rg_lru_kernel<T><<<grid, block, 0, stream>>>(params);
-}
-
-#define INSTANTIATE_RGLRU_DATA_TYPE(T) template void invokeRGLRU<T>(lruParams & params, cudaStream_t stream);
-
-INSTANTIATE_RGLRU_DATA_TYPE(float);
-INSTANTIATE_RGLRU_DATA_TYPE(half);
-#ifdef ENABLE_BF16
-INSTANTIATE_RGLRU_DATA_TYPE(__nv_bfloat16);
-#endif
-#undef INSTANTIATE_RGLRU_DATA_TYPE
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename T>
-__launch_bounds__(128, 2) __global__ void rg_lru_update_kernel(lruParams params)
-{
-    T* output = reinterpret_cast<T*>(params.out_ptr);
-    float* state = reinterpret_cast<float*>(params.state_ptr);
-    T* x = reinterpret_cast<T*>(params.x_ptr);
-    T* y = reinterpret_cast<T*>(params.y_ptr);
-    T* y_bias = reinterpret_cast<T*>(params.y_bias_ptr);
-    T* A = reinterpret_cast<T*>(params.A_ptr);
-    int num_channels = params.width;
-    int block_size = params.block_size;
-
-    bool enable_fuse_gate = (params.gate_ptr != nullptr);
-    bool enable_gate_bias;
-    T *gate_x, *gate_a, *gate_x_bias, *gate_a_bias;
-    if (enable_fuse_gate)
-    {
-        enable_gate_bias = (params.gate_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-        }
-    }
-    else
-    {
-        enable_gate_bias = (params.gate_x_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_x_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_a_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_x_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_a_bias_ptr);
-        }
-    }
-
-    int const channel = blockIdx.x * blockDim.x + threadIdx.x;
-    if (channel >= num_channels)
-        return;
-    int const sample = blockIdx.y; // batch id
-    int const slot_idx = params.slot_mapping_ptr == nullptr ? sample : params.slot_mapping_ptr[sample];
-    int const idx = sample * num_channels + channel;
-    int const gate_num_channels = enable_fuse_gate ? num_channels * 2 : num_channels;
-    int const gate_base_idx = sample * gate_num_channels;
-    int const gx_dim_idx = enable_fuse_gate ? channel / block_size * block_size * 2 + channel % block_size : channel;
-    int const ga_dim_idx = enable_fuse_gate ? gx_dim_idx + block_size : channel;
-
-    float state_reg = state[slot_idx * num_channels + channel];
-
-    // Read a
-    float param_a = cuda_cast<float>(A[channel]);
-    float c = param_a <= 20.f ? -8.0f * __logf(1.0f + __expf(param_a)) : -8.0f * param_a;
-
-    // Read y
-    float y_reg;
-    if (y_bias)
-    {
-        y_reg = cuda_cast<float>(y[idx] + y_bias[channel]);
-        // GELU
-        float k0 = float(0.7978845608028654);
-        float k1 = float(0.044715);
-        float y_tanh = k0 * y_reg * (1.0 + k1 * y_reg * y_reg);
-        float exp_val = -1.f * cuda_abs(y_tanh * 2);
-        y_reg = 0.5f * y_reg
-            * (1.f + copysignf_pos(__fdividef((1.f - __expf(exp_val)), (1.f + __expf(exp_val))), y_tanh));
-    }
-    else if (y)
-    {
-        y_reg = cuda_cast<float>(y[idx]);
-    }
-    else
-    {
-        y_reg = 1.f;
-    }
-    // Read gate_x
-    float gate_x_reg, gate_a_reg;
-    if (enable_gate_bias)
-    {
-        gate_x_reg = cuda_cast<float>(-gate_x[gate_base_idx + gx_dim_idx] - gate_x_bias[gx_dim_idx]);
-        gate_a_reg = cuda_cast<float>(-gate_a[gate_base_idx + ga_dim_idx] - gate_a_bias[ga_dim_idx]);
-    }
-    else
-    {
-        gate_x_reg = cuda_cast<float>(-gate_x[gate_base_idx + gx_dim_idx]);
-        gate_a_reg = cuda_cast<float>(-gate_a[gate_base_idx + ga_dim_idx]);
-    }
-    // Get gated inputs
-    float sigmoid_x = __fdividef(1.0f, (1.0f + __expf(gate_x_reg)));
-    float sigmoid_a = __fdividef(1.0f, (1.0f + __expf(gate_a_reg)));
-    float log_a = sigmoid_a * c;
-    float a = __expf(log_a);
-    float a_square = __expf(2.0 * log_a);
-    float outf = y_reg;
-    float normalized_x = cuda_cast<float>(x[idx]) * sigmoid_x * sqrtf(1 - a_square);
-
-    // RNN update
-    state_reg = a * state_reg + normalized_x;
-    outf *= state_reg;
-
-    // Write output and state
-    output[sample * num_channels + channel] = cuda_cast<T>(outf);
-    state[slot_idx * num_channels + channel] = state_reg;
-}
-
-template <typename T>
-void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream)
-{
-    int samples = params.batch;
-    int channels = params.width;
-
-    int const threads = 128;
-    int const blocks = (channels + threads - 1) / threads;
-    dim3 block(threads, 1);
-    dim3 grid(blocks, samples);
-
-    rg_lru_update_kernel<T><<<grid, block, 0, stream>>>(params);
-}
-
-#define INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(T)                                                                          \
-    template void invokeRGLRUUpdate<T>(lruParams & params, cudaStream_t stream)
-
-INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(float);
-INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(half);
-#ifdef ENABLE_BF16
-INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(__nv_bfloat16);
-#endif
-#undef INSTANTIATE_RGLRU_UPDATE_DATA_TYPE
-
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/lruKernel.h b/cpp/tensorrt_llm/kernels/lruKernel.h
index c49f039d..48156fb7 100644
--- a/cpp/tensorrt_llm/kernels/lruKernel.h
+++ b/cpp/tensorrt_llm/kernels/lruKernel.h
@@ -16,6 +16,7 @@
 
 #pragma once
 
+#include <cuda_runtime.h>
 #include "tensorrt_llm/common/assert.h"
 
 namespace tensorrt_llm
@@ -49,11 +50,11 @@ struct lruParams
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-void invokeRGLRU(lruParams& params, cudaStream_t stream);
+// template <typename T>
+// void invokeRGLRU(lruParams& params, cudaStream_t stream);
 
-template <typename T>
-void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);
+// template <typename T>
+// void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);
 
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu
index 1630319b..fea208ea 100644
--- a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu
+++ b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu
@@ -1302,6 +1302,7 @@ void loraAddBias(T* output, T const* lora_result, BiasType const* bias, int64_t
         output, lora_result, bias, num_valid_tokens_ptr, permuted_experts, inter_size);
 }
 
+#ifdef ENABLE_FP8
 template <class BiasType>
 void loraAddBias(__nv_fp8_e4m3* output, __nv_fp8_e4m3 const* lora_result, BiasType const* bias,
     int64_t const* num_valid_tokens_ptr, int64_t inter_size, int* permuted_experts, int64_t num_tokens,
@@ -1309,6 +1310,7 @@ void loraAddBias(__nv_fp8_e4m3* output, __nv_fp8_e4m3 const* lora_result, BiasTy
 {
     TLLM_THROW("Lora is not supported for fp8.");
 }
+#endif
 
 template <class T>
 __global__ void loraReorderKernel(
@@ -1938,8 +1940,12 @@ void CutlassMoeFCRunner<T, WeightType, OutputType, ScaleBiasType, Enable>::runMo
 {
     static constexpr bool int_scales_required
         = std::is_same<WeightType, uint8_t>::value || std::is_same<WeightType, cutlass::uint4b_t>::value;
+#ifdef ENABLE_FP8
     static constexpr bool fp8_scales_required
         = std::is_same<WeightType, __nv_fp8_e4m3>::value || std::is_same<WeightType, __nv_fp8_e5m2>::value;
+#else
+    static constexpr bool fp8_scales_required = false;
+#endif
     static constexpr bool is_supported_lora = std::is_same<T, ScaleBiasType>::value && !use_fp8;
 
     auto const* input_activations = static_cast<T const*>(input_activations_void);
@@ -2285,9 +2291,13 @@ std::vector<size_t> GemmProfilerBackend::getProfilerWorkspaces(int maxM, bool is
     float weight_bytes
         = mWType == nvinfer1::DataType::kINT4 ? 0.5f : static_cast<float>(tensorrt_llm::common::getDTypeSize(mWType));
     size_t output_bytes = tensorrt_llm::common::getDTypeSize(mOType);
+#ifdef ENABLE_FP8
     size_t gemm_output_bytes = (mOType == nvinfer1::DataType::kFP8)
         ? sizeof(HopperGroupedGemmInput::OutputTypeAdaptor_t<__nv_fp8_e4m3>)
         : output_bytes;
+#else
+    size_t gemm_output_bytes = output_bytes;
+#endif
 
     size_t hidden_size = mExpertHiddenSize;
     size_t inter_size = mExpertInterSize; // Already divided by TP
diff --git a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h
index 16a8f3b4..92a25b72 100644
--- a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h
+++ b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h
@@ -263,7 +263,7 @@ class CutlassMoeFCRunner : public CutlassMoeFCRunnerInterface
     // This should leave the variable unchanged in any currently supported configuration
     using UnfusedGemmOutputType = typename HopperGroupedGemmInput::OutputTypeAdaptor_t<OutputType>;
 
-    static_assert(!std::is_same_v<OutputType, __nv_fp8_e4m3>, "Current logic requires output type to be non-FP8");
+    // static_assert(!std::is_same_v<OutputType, __nv_fp8_e4m3>, "Current logic requires output type to be non-FP8");
     // We introduce this as a separate parameter, so that if we ever remove the above condition we can decouple
     // ScaleBiasType and OutputType easily. For now these are required to be equivalent
     static_assert(std::is_same_v<OutputType, ScaleBiasType>, "Scale and bias types must match OutputType");
diff --git a/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h b/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h
index 1893407c..be5828d9 100644
--- a/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h
+++ b/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h
@@ -68,6 +68,7 @@ constexpr int32_t kSM_72 = 72;
 constexpr int32_t kSM_75 = 75;
 constexpr int32_t kSM_80 = 80;
 constexpr int32_t kSM_86 = 86;
+constexpr int32_t kSM_87 = 87;
 constexpr int32_t kSM_89 = 89;
 constexpr int32_t kSM_90 = 90;
 
diff --git a/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h b/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h
index 5ac02cff..637203b9 100644
--- a/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h
+++ b/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda_fp16.h>
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
 #include <cuda_runtime.h>
 #include <cuda_runtime_api.h>
 
diff --git a/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu b/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu
index 21891c35..741daeb9 100644
--- a/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu
+++ b/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu
@@ -16,7 +16,7 @@
 
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
@@ -1124,21 +1124,22 @@ __global__ void airTopPSampling(Counter<T, IdxT, AccT>* counters, HisT* histogra
             }
             __syncthreads();
         }
-
+        
+        // TODO: check when will this kernel be used
         // Acquire the summation of each 32 buckets
         for (int i = threadIdx.x; i < numBuckets; i += BlockSize)
         {
-            reduce_store_async(warp, warpSum + i / WARP_SIZE, histPtr[i], cg::plus<float>{});
+            // reduce_store_async(warp, warpSum + i / WARP_SIZE, histPtr[i], cg::plus<float>{});
         }
         __syncthreads();
 
         // Acquire the summation of all the 2048 buckets
         if (threadIdx.x < WARP_SIZE)
         {
-            reduce_store_async(warp, blockSum, warpSum[threadIdx.x], cg::plus<float>{});
+            // reduce_store_async(warp, blockSum, warpSum[threadIdx.x], cg::plus<float>{});
             if constexpr (BitsPerPass == 11)
             {
-                reduce_update_async(warp, blockSum, warpSum[threadIdx.x + WARP_SIZE], cg::plus<float>{});
+                // reduce_update_async(warp, blockSum, warpSum[threadIdx.x + WARP_SIZE], cg::plus<float>{});
             }
         }
         __syncthreads();
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan.cu b/cpp/tensorrt_llm/kernels/selectiveScan.cu
index 064b10cc..b85da6c2 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan.cu
+++ b/cpp/tensorrt_llm/kernels/selectiveScan.cu
@@ -29,12 +29,14 @@
 
 #include "selectiveScan.h"
 
+#if 0
 #include "selectiveScan/CudaType.h"
 #include "selectiveScan/bmmchunk.h"
 #include "selectiveScan/chunkcumsum.h"
 #include "selectiveScan/chunkscan.h"
 #include "selectiveScan/chunkstate.h"
 #include "selectiveScan/statepassing.h"
+#endif 
 
 namespace tensorrt_llm
 {
@@ -340,6 +342,7 @@ void invokeSelectiveScan(SSMParamsBase& params, cudaStream_t stream)
 template <typename input_t, typename weight_t>
 void invokeChunkScan(SSMParamsBase& params, cudaStream_t stream, tensorrt_llm::common::CUDADriverWrapper* driver)
 {
+#if 0
     int B = params.batch;
     int L = params.max_seqlen;
     int H = params.nheads;
@@ -436,6 +439,7 @@ void invokeChunkScan(SSMParamsBase& params, cudaStream_t stream, tensorrt_llm::c
     cudaFuncSetAttribute(chunk_scan, cudaFuncAttributeMaxDynamicSharedMemorySize, shms[4]);
     chunk_scan<<<bds[4], tds[4], shms[4], stream>>>(
         B, L, H, P, G, N, mxY, mxOs, mxdc, mxdA, mxCB, mxD, (useTmas[4] ? &descs[4] : mxXBC), mxZ, rp, ltip);
+#endif
 }
 
 #define INSTANTIATE_SELECTIVE_SCAN_DATA_TYPE(input_t, weight_t)                                                        \
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h b/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h
index 66f8182e..6171d4e3 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h
@@ -16,15 +16,18 @@
 
 #pragma once
 
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+typedef __nv_fp8_e4m3 e4m3_t;
+typedef __nv_fp8_e5m2 e5m2_t;
+#endif
+
 #include <mma.h>
 
 typedef __nv_half fp16_t;
 typedef __nv_bfloat16 bf16_t;
 typedef float fp32_t;
 typedef double fp64_t;
-typedef __nv_fp8_e4m3 e4m3_t;
-typedef __nv_fp8_e5m2 e5m2_t;
 
 enum CudaType
 {
@@ -32,8 +35,10 @@ enum CudaType
     CT_BF16,
     CT_FP32,
     CT_FP64,
+#ifdef ENABLE_FP8
     CT_E4M3,
     CT_E5M2,
+#endif
 };
 
 template <CudaType>
@@ -63,6 +68,7 @@ struct EnToTp<CT_FP64>
     typedef fp64_t type;
 };
 
+#ifdef ENABLE_FP8
 template <>
 struct EnToTp<CT_E4M3>
 {
@@ -74,6 +80,7 @@ struct EnToTp<CT_E5M2>
 {
     typedef e5m2_t type;
 };
+#endif
 
 template <CudaType en_>
 using EnToTp_t = typename EnToTp<en_>::type;
@@ -105,6 +112,7 @@ struct TpToEn<fp64_t>
     static constexpr CudaType value = CT_FP64;
 };
 
+#ifdef ENABLE_FP8
 template <>
 struct TpToEn<e4m3_t>
 {
@@ -116,6 +124,7 @@ struct TpToEn<e5m2_t>
 {
     static constexpr CudaType value = CT_E5M2;
 };
+#endif
 
 template <class Tp_>
 constexpr CudaType TpToEn_v = TpToEn<Tp_>::value;
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h b/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h
index e1a62cdd..7e2eb0e7 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h
@@ -17,7 +17,11 @@
 #pragma once
 
 #include <cuda.h>
+
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
+
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h b/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h
index ac4db9d9..9db92d77 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h
@@ -16,7 +16,10 @@
 
 #pragma once
 
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
+
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h b/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h
index 28cd7fde..543401ba 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda.h>
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h b/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h
index 030f99a2..1f32ca32 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda.h>
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h b/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h
index 45cfced4..d7ee5b68 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h
@@ -16,7 +16,9 @@
 
 #pragma once
 
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu
index 7eafc17a..8e33d206 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu
@@ -1831,7 +1831,7 @@ __global__ void shiftKCache(KVCacheBuffer kvCacheBuffer, KVLinearBuffer shiftKCa
     // Use 8bit cache.
     static constexpr bool ENABLE_8BITS_CACHE = sizeof(T_cache) == 1;
     // FP8 KV Cache.
-    [[maybe_unused]] static constexpr bool FP8_K_CACHE = std::is_same<T_cache, __nv_fp8_e4m3>::value;
+    [[maybe_unused]] static constexpr bool FP8_K_CACHE = false; // std::is_same<T_cache, __nv_fp8_e4m3>::value;
     // INT8 KV Cache.
     static constexpr bool INT8_K_CACHE = std::is_same<T_cache, int8_t>::value;
 
@@ -2127,8 +2127,10 @@ void invokeConversion(Dst* dst, Src const* src, int64_t size, float const* __res
 #define INSTANTIATE_invokeConversion(Dst, Src)                                                                         \
     template void invokeConversion<Dst, Src>(                                                                          \
         Dst * dst, Src const* src, int64_t size, float const* __restrict__ scale, cudaStream_t stream)
+#ifdef ENABLE_FP8
 INSTANTIATE_invokeConversion(__nv_fp8_e4m3, half);
 INSTANTIATE_invokeConversion(__nv_fp8_e4m3, __nv_bfloat16);
+#endif
 #undef INSTANTIATE_invokeConversion
 
 } // namespace kernels
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu
index 14645db9..35951430 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu
@@ -22,7 +22,7 @@ namespace tensorrt_llm
 namespace kernels
 {
 
-#ifdef ENABLE_BF16
+#if defined(ENABLE_BF16) && defined(ENABLE_FP8)
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(__nv_bfloat16, __nv_fp8_e4m3, KVBlockArray);
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(__nv_bfloat16, __nv_fp8_e4m3, KVLinearBuffer);
 #endif
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu
index 8d48ecae..3a33fb0d 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu
@@ -21,9 +21,9 @@ namespace tensorrt_llm
 {
 namespace kernels
 {
-
+#if defined(ENABLE_FP8)
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(float, __nv_fp8_e4m3, KVBlockArray);
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(float, __nv_fp8_e4m3, KVLinearBuffer);
-
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu
index b1fe8d13..f43e7be5 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu
@@ -21,9 +21,9 @@ namespace tensorrt_llm
 {
 namespace kernels
 {
-
+#if defined(ENABLE_FP8)
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(half, __nv_fp8_e4m3, KVBlockArray);
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(half, __nv_fp8_e4m3, KVLinearBuffer);
-
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h
index 175bfdd6..1211eec6 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h
@@ -51,7 +51,7 @@ struct Rotary_vec_t
     using Type = T;
     using BaseType = T;
     // Quantized output type only supports fp8 currently.
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 1;
 };
 
@@ -62,7 +62,7 @@ struct Rotary_vec_t<float, 32>
 {
     using Type = float;
     using BaseType = float;
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 1;
 };
 
@@ -71,7 +71,7 @@ struct Rotary_vec_t<float, 64>
 {
     using Type = float2;
     using BaseType = float;
-    using QuantizedType = mmha::fp8_2_t;
+    using QuantizedType = void; // mmha::fp8_2_t;
     static constexpr int size = 2;
 };
 
@@ -80,7 +80,7 @@ struct Rotary_vec_t<float, 128>
 {
     using Type = float4;
     using BaseType = float;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     static constexpr int size = 4;
 };
 
@@ -89,7 +89,7 @@ struct Rotary_vec_t<float, 256>
 {
     using Type = mmha::Float8_;
     using BaseType = float;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     static constexpr int size = 8;
 };
 
@@ -100,7 +100,7 @@ struct Rotary_vec_t<half, 32>
 {
     using Type = uint16_t;
     using BaseType = uint16_t;
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 2;
 };
 
@@ -109,7 +109,7 @@ struct Rotary_vec_t<half, 64>
 {
     using Type = uint32_t;
     using BaseType = uint16_t;
-    using QuantizedType = mmha::fp8_2_t;
+    using QuantizedType = void; // mmha::fp8_2_t;
     static constexpr int size = 2;
 };
 
@@ -118,7 +118,7 @@ struct Rotary_vec_t<half, 128>
 {
     using Type = uint2;
     using BaseType = uint16_t;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     static constexpr int size = 4;
 };
 
@@ -127,7 +127,7 @@ struct Rotary_vec_t<half, 256>
 {
     using Type = uint4;
     using BaseType = uint16_t;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     static constexpr int size = 8;
 };
 
@@ -140,7 +140,7 @@ struct Rotary_vec_t<__nv_bfloat16, 32>
 {
     using Type = __nv_bfloat16;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 1;
 };
 
@@ -149,7 +149,7 @@ struct Rotary_vec_t<__nv_bfloat16, 64>
 {
     using Type = __nv_bfloat162;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = mmha::fp8_2_t;
+    using QuantizedType = void; // mmha::fp8_2_t;
     static constexpr int size = 2;
 };
 
@@ -158,7 +158,7 @@ struct Rotary_vec_t<__nv_bfloat16, 128>
 {
     using Type = mmha::bf16_4_t;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     static constexpr int size = 4;
 };
 
@@ -167,7 +167,7 @@ struct Rotary_vec_t<__nv_bfloat16, 256>
 {
     using Type = mmha::bf16_8_t;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     static constexpr int size = 8;
 };
 
@@ -335,8 +335,8 @@ __global__ void applyBiasRopeUpdateKVCache(QKVPreprocessingParams<T, KVCacheBuff
     // The base type will share the rotary coefficient.
     using BaseType = typename Rotary_vec_t<T, Dh_MAX>::BaseType;
     // Quantized output only supports fp8 currently.
-    using QuantizedEltType = __nv_fp8_e4m3;
-    using QuantizedVecType = typename Rotary_vec_t<T, Dh_MAX>::QuantizedType;
+    // using QuantizedEltType = __nv_fp8_e4m3;
+    // using QuantizedVecType = typename Rotary_vec_t<T, Dh_MAX>::QuantizedType;
     // GPTJ rotary embedding: two elements share the same rotary coefficient.
     constexpr int ROTARY_COEF_VEC_SIZE = ROTARY_TYPE == RotaryPositionEmbeddingType::GPTJ ? VEC_SIZE / 2 : VEC_SIZE;
 
@@ -507,41 +507,41 @@ __global__ void applyBiasRopeUpdateKVCache(QKVPreprocessingParams<T, KVCacheBuff
                 VecType k_to_cache = params.position_shift_enabled ? k_wo_pos : k;
 
                 auto const dst_q_idx = static_cast<size_t>(global_token_idx) * params.q_hidden_size + hidden_idx;
-                QuantizedVecType* quantized_q_ptr = STORE_QKV
-                    ? reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_q_idx)
-                    : reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.Q, dst_q_idx);
+                // QuantizedVecType* quantized_q_ptr = STORE_QKV
+                //     ? reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_q_idx)
+                //     : reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.Q, dst_q_idx);
                 VecType* q_ptr = STORE_QKV ? reinterpret_ptr<T, VecType>(params.QKV, src_q_idx)
                                            : reinterpret_ptr<T, VecType>(params.Q, dst_q_idx);
 
-                if (params.quantized_fp8_output)
-                {
-                    // use 1.0f scale currently for qkv input of FP8 FMHA.
-                    mmha::convert_to_fp8(quantized_q_ptr, q);
-                }
-                else
-                {
+                // if (params.quantized_fp8_output)
+                // {
+                //     // use 1.0f scale currently for qkv input of FP8 FMHA.
+                //     mmha::convert_to_fp8(quantized_q_ptr, q);
+                // }
+                // else
+                // {
                     *q_ptr = q;
-                }
+                // }
                 if ((params.head_num == params.kv_head_num) || (head_idx == (kv_head_idx * params.qheads_per_kv_head)))
                 {
                     if constexpr (STORE_QKV)
                     {
-                        if (params.quantized_fp8_output)
-                        {
-                            // use 1.0f scale currently for qkv input of FP8 FMHA.
-                            mmha::convert_to_fp8(
-                                reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_k_idx), k);
-                            mmha::convert_to_fp8(
-                                reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_v_idx), v);
-                        }
-                        else
-                        {
+                        // if (params.quantized_fp8_output)
+                        // {
+                        //     // use 1.0f scale currently for qkv input of FP8 FMHA.
+                        //     mmha::convert_to_fp8(
+                        //         reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_k_idx), k);
+                        //     mmha::convert_to_fp8(
+                        //         reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_v_idx), v);
+                        // }
+                        // else
+                        // {
                             *reinterpret_cast<VecType*>(&params.QKV[src_k_idx]) = k;
                             if constexpr (ADD_BIAS)
                             {
                                 *reinterpret_cast<VecType*>(&params.QKV[src_v_idx]) = v;
                             }
-                        }
+                        // }
                     }
 
                     if (valid_kv_cache_pos)
@@ -577,7 +577,7 @@ template <typename T>
 struct VecType
 {
     using Type = T;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     using GPTNeoXEltType = T;
     using GPTJEltType = T;
 };
@@ -586,7 +586,7 @@ template <>
 struct VecType<float>
 {
     using Type = float4;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     using GPTNeoXEltType = float;
     using GPTJEltType = float2;
 };
@@ -595,7 +595,7 @@ template <>
 struct VecType<half>
 {
     using Type = uint4;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     using GPTNeoXEltType = uint16_t;
     using GPTJEltType = uint32_t;
 };
@@ -604,7 +604,7 @@ template <>
 struct VecType<__nv_bfloat16>
 {
     using Type = mmha::bf16_8_t;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     using GPTNeoXEltType = __nv_bfloat16;
     using GPTJEltType = __nv_bfloat162;
 };
@@ -643,8 +643,8 @@ __global__ void applyBiasRopeUpdateKVCacheV2(QKVPreprocessingParams<T, KVCacheBu
     // Constants.
     using VecT = typename VecType<T>::Type;
     // Quantized output only supports fp8 currently.
-    using QuantizedEltType = __nv_fp8_e4m3;
-    using QuantizedVecType = typename VecType<T>::QuantizedType;
+    // using QuantizedEltType = __nv_fp8_e4m3;
+    // using QuantizedVecType = typename VecType<T>::QuantizedType;
     using GPTNeoXEltT = typename VecType<T>::GPTNeoXEltType;
     using GPTJEltT = typename VecType<T>::GPTJEltType;
     constexpr auto HEAD_SIZE = Dh;
@@ -808,43 +808,43 @@ __global__ void applyBiasRopeUpdateKVCacheV2(QKVPreprocessingParams<T, KVCacheBu
         if (valid_token)
         {
             auto const dst_q_idx = static_cast<size_t>(global_token_idx) * params.q_hidden_size + hidden_idx;
-            QuantizedVecType* quantized_q_ptr = STORE_QKV
-                ? reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_q_idx)
-                : reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.Q, dst_q_idx);
+            // QuantizedVecType* quantized_q_ptr = STORE_QKV
+            //     ? reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.QuantizedQKV, src_q_idx)
+            //     : reinterpret_ptr<QuantizedEltType, QuantizedVecType>(params.Q, dst_q_idx);
             VecT* q_ptr = STORE_QKV ? reinterpret_ptr<T, VecT>(params.QKV, src_q_idx)
                                     : reinterpret_ptr<T, VecT>(params.Q, dst_q_idx);
 
-            if (params.quantized_fp8_output)
-            {
-                // use 1.0f scale currently for qkv input of FP8 FMHA.
-                mmha::convert_to_fp8(quantized_q_ptr, q);
-            }
-            else
-            {
+            // if (params.quantized_fp8_output)
+            // {
+            //     // use 1.0f scale currently for qkv input of FP8 FMHA.
+            //     mmha::convert_to_fp8(quantized_q_ptr, q);
+            // }
+            // else
+            // {
                 *q_ptr = q;
-            }
+            // }
             if ((params.head_num == params.kv_head_num) || (head_idx == (kv_head_idx * params.qheads_per_kv_head)))
             {
                 if constexpr (STORE_QKV)
                 {
-                    if (params.quantized_fp8_output)
-                    {
-                        // use 1.0f scale currently for qkv input of FP8 FMHA.
-                        mmha::convert_to_fp8(reinterpret_cast<QuantizedVecType*>(
-                                                 reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_k_idx),
-                            k);
-                        mmha::convert_to_fp8(reinterpret_cast<QuantizedVecType*>(
-                                                 reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_v_idx),
-                            v);
-                    }
-                    else
-                    {
+                    // if (params.quantized_fp8_output)
+                    // {
+                    //     // use 1.0f scale currently for qkv input of FP8 FMHA.
+                    //     mmha::convert_to_fp8(reinterpret_cast<QuantizedVecType*>(
+                    //                              reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_k_idx),
+                    //         k);
+                    //     mmha::convert_to_fp8(reinterpret_cast<QuantizedVecType*>(
+                    //                              reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_v_idx),
+                    //         v);
+                    // }
+                    // else
+                    // {
                         *reinterpret_cast<VecT*>(&params.QKV[src_k_idx]) = k;
                         if constexpr (ADD_BIAS)
                         {
                             *reinterpret_cast<VecT*>(&params.QKV[src_v_idx]) = v;
                         }
-                    }
+                    // }
                 }
 
                 if (valid_kv_cache_pos)
diff --git a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu
index d131488c..e39b5a11 100644
--- a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu
+++ b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu
@@ -159,6 +159,7 @@ bool cudaCoreGemmDispatcher(Params const& params, cudaStream_t stream)
     {
         dispatched = false;
     }
+#if defined(ENABLE_FP8)
     else if (params.inputType == nvinfer1::DataType::kFP8)
     {
         if (params.k % 16 != 0)
@@ -183,6 +184,7 @@ bool cudaCoreGemmDispatcher(Params const& params, cudaStream_t stream)
             dispatched = false;
         }
     }
+#endif
     else if (params.inputType == nvinfer1::DataType::kHALF)
     {
         if (params.k % 8 != 0)
diff --git a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h
index 4daad747..5735c846 100644
--- a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h
+++ b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h
@@ -28,7 +28,9 @@
 #include <cstdint>
 #include <cuda_bf16.h>
 #include <cuda_fp16.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <cuda_runtime.h>
 #include <cuda_runtime_api.h>
 #include <iostream>
diff --git a/cpp/tensorrt_llm/plugins/CMakeLists.txt b/cpp/tensorrt_llm/plugins/CMakeLists.txt
index 045a34d1..2a34eb0f 100755
--- a/cpp/tensorrt_llm/plugins/CMakeLists.txt
+++ b/cpp/tensorrt_llm/plugins/CMakeLists.txt
@@ -50,9 +50,9 @@ set(PLUGIN_LISTS
     lookupPlugin
     loraPlugin
     mixtureOfExperts
-    selectiveScanPlugin
+    # selectiveScanPlugin
     mambaConv1dPlugin
-    lruPlugin
+    # lruPlugin
     cumsumLastDimPlugin)
 
 foreach(PLUGIN_ITER ${PLUGIN_LISTS})
@@ -129,7 +129,7 @@ target_link_libraries(
   ${CUBLASLT_LIB}
   nvinfer
   ${CUDA_DRV_LIB}
-  ${CUDA_NVML_LIB}
+  # ${CUDA_NVML_LIB}
   ${CUDA_RT_LIB}
   ${CMAKE_DL_LIBS}
   ${SHARED_TARGET})
diff --git a/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp b/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp
index 3508b2b4..d1571e88 100644
--- a/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp
+++ b/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp
@@ -195,9 +195,9 @@ extern "C"
         static tensorrt_llm::plugins::WeightOnlyQuantMatmulPluginCreator weightOnlyQuantMatmulPluginCreator;
         static tensorrt_llm::plugins::LookupPluginCreator lookupPluginCreator;
         static tensorrt_llm::plugins::LoraPluginCreator loraPluginCreator;
-        static tensorrt_llm::plugins::SelectiveScanPluginCreator selectiveScanPluginCreator;
+        // static tensorrt_llm::plugins::SelectiveScanPluginCreator selectiveScanPluginCreator;
         static tensorrt_llm::plugins::MambaConv1dPluginCreator mambaConv1DPluginCreator;
-        static tensorrt_llm::plugins::lruPluginCreator lruPluginCreator;
+        // static tensorrt_llm::plugins::lruPluginCreator lruPluginCreator;
         static tensorrt_llm::plugins::CumsumLastDimPluginCreator cumsumLastDimPluginCreator;
 
         static std::array pluginCreators
@@ -224,9 +224,9 @@ extern "C"
                   creatorPtr(weightOnlyQuantMatmulPluginCreator),
                   creatorPtr(lookupPluginCreator),
                   creatorPtr(loraPluginCreator),
-                  creatorPtr(selectiveScanPluginCreator),
+                //   creatorPtr(selectiveScanPluginCreator),
                   creatorPtr(mambaConv1DPluginCreator),
-                  creatorPtr(lruPluginCreator),
+                //   creatorPtr(lruPluginCreator),
                   creatorPtr(cumsumLastDimPluginCreator),
               };
         nbCreators = pluginCreators.size();
diff --git a/cpp/tensorrt_llm/plugins/common/plugin.cpp b/cpp/tensorrt_llm/plugins/common/plugin.cpp
index 95401ade..0fc922a6 100644
--- a/cpp/tensorrt_llm/plugins/common/plugin.cpp
+++ b/cpp/tensorrt_llm/plugins/common/plugin.cpp
@@ -23,7 +23,9 @@
 #include <cstdint>
 #include <cuda_bf16.h>
 #include <cuda_fp16.h>
+#ifdef ENABLE_FP8
 #include <cuda_fp8.h>
+#endif
 #include <functional>
 #include <mutex>
 #include <thread>
diff --git a/cpp/tensorrt_llm/plugins/common/plugin.h b/cpp/tensorrt_llm/plugins/common/plugin.h
index 96bd1ef4..2884416d 100644
--- a/cpp/tensorrt_llm/plugins/common/plugin.h
+++ b/cpp/tensorrt_llm/plugins/common/plugin.h
@@ -33,7 +33,7 @@
 #include <cstring>
 #include <map>
 #include <memory>
-#include <nvml.h>
+// #include <nvml.h>
 #include <optional>
 #include <set>
 #include <string>
@@ -302,13 +302,13 @@ private:
     std::unordered_map<std::string_view, Record> mMap;
 };
 
-#define NVML_CHECK(cmd)                                                                                                \
-    do                                                                                                                 \
-    {                                                                                                                  \
-        nvmlReturn_t r = cmd;                                                                                          \
-        if (r != NVML_SUCCESS)                                                                                         \
-        {                                                                                                              \
-            printf("Failed, NVML error %s:%d '%s'\n", __FILE__, __LINE__, nvmlErrorString(r));                         \
-            exit(EXIT_FAILURE);                                                                                        \
-        }                                                                                                              \
-    } while (0)
+// #define NVML_CHECK(cmd)                                                                                                \
+//     do                                                                                                                 \
+//     {                                                                                                                  \
+//         nvmlReturn_t r = cmd;                                                                                          \
+//         if (r != NVML_SUCCESS)                                                                                         \
+//         {                                                                                                              \
+//             printf("Failed, NVML error %s:%d '%s'\n", __FILE__, __LINE__, nvmlErrorString(r));                         \
+//             exit(EXIT_FAILURE);                                                                                        \
+//         }                                                                                                              \
+//     } while (0)
diff --git a/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp b/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp
index 3b2142c8..27966e74 100644
--- a/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp
+++ b/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp
@@ -147,7 +147,9 @@ void GemmSwigluPlugin::init(nvinfer1::DataType type)
     mType = type;
     if (mType == nvinfer1::DataType::kFP8)
     {
+        #if defined(ENABLE_FP8)
         mGemmRunner = std::make_shared<CutlassFusedGatedGemmRunner<__nv_fp8_e4m3>>();
+        #endif
     }
     else
     {
diff --git a/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp b/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp
index 29190010..4f46f44b 100644
--- a/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp
+++ b/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp
@@ -827,8 +827,10 @@ int GPTAttentionPluginCommon::enqueueContext(EnqueueContextParams<T, KVCacheBuff
     T* qk_buf_ = reinterpret_cast<T*>(nextWorkspacePtr(workspace_byte_ptr, offset, qk_buf_size));
     T* qkv_buf_2_ = reinterpret_cast<T*>(nextWorkspacePtr(workspace_byte_ptr, offset, qkv_buf_2_size));
     float* qk_buf_float_ = reinterpret_cast<float*>(nextWorkspacePtr(workspace_byte_ptr, offset, qk_buf_float_size));
-    __nv_fp8_e4m3* fp8_qkv_buffer
-        = reinterpret_cast<__nv_fp8_e4m3*>(nextWorkspacePtr(workspace_byte_ptr, offset, fp8_qkv_buffer_size));
+    // __nv_fp8_e4m3* fp8_qkv_buffer
+    //     = reinterpret_cast<__nv_fp8_e4m3*>(nextWorkspacePtr(workspace_byte_ptr, offset, fp8_qkv_buffer_size));
+    T* fp8_qkv_buffer
+        = reinterpret_cast<T*>(nextWorkspacePtr(workspace_byte_ptr, offset, fp8_qkv_buffer_size));
     int* padding_offset = mEnableContextFMHA
         ? nullptr
         : reinterpret_cast<int*>(nextWorkspacePtr(workspace_byte_ptr, offset, padding_offset_size));
diff --git a/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp b/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp
index ad4e3973..888fd72c 100644
--- a/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp
+++ b/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp
@@ -153,6 +153,7 @@ void WeightOnlyGroupwiseQuantMatmulPlugin::init(nvinfer1::DataType type, int qua
             {
                 TLLM_THROW("W4A(fp)8 kernel is unsupported on pre-Ada (sm<89) architectures!");
             }
+            #if defined(ENABLE_FP8)
             if (quant_algo & ZERO)
             {
                 // has zeros
@@ -167,6 +168,7 @@ void WeightOnlyGroupwiseQuantMatmulPlugin::init(nvinfer1::DataType type, int qua
                     = std::make_shared<tensorrt_llm::kernels::cutlass_kernels::CutlassFpAIntBGemmRunner<__nv_fp8_e4m3,
                         cutlass::uint4b_t, cutlass::WeightOnlyQuantOp::FINEGRAINED_SCALE_ONLY, half, half, half>>();
             }
+            #endif
         }
         else
         {
@@ -387,9 +389,11 @@ int WeightOnlyGroupwiseQuantMatmulPlugin::enqueue(nvinfer1::PluginTensorDesc con
         {
             if (mQuantAlgo & FP8_ALPHA)
             {
+                #if defined(ENABLE_FP8)
                 tensorrt_llm::kernels::apply_per_channel_scale_kernel_launcher<half, __nv_fp8_e4m3>(
                     reinterpret_cast<__nv_fp8_e4m3*>(workspace), reinterpret_cast<half const*>(inputs[0]),
                     reinterpret_cast<half const*>(inputs[mPreQuantScaleInputIdx]), m, k, stream);
+                #endif
             }
             else
             {
@@ -403,9 +407,11 @@ int WeightOnlyGroupwiseQuantMatmulPlugin::enqueue(nvinfer1::PluginTensorDesc con
         {
             if (mQuantAlgo & FP8_ALPHA)
             {
+                #if defined(ENABLE_FP8)
                 tensorrt_llm::kernels::apply_per_channel_scale_kernel_launcher<__nv_bfloat16, __nv_fp8_e4m3>(
                     reinterpret_cast<__nv_fp8_e4m3*>(workspace), reinterpret_cast<__nv_bfloat16 const*>(inputs[0]),
                     reinterpret_cast<__nv_bfloat16 const*>(inputs[mPreQuantScaleInputIdx]), m, k, stream);
+                #endif
             }
             else
             {
diff --git a/cpp/tensorrt_llm/runtime/gptSession.cpp b/cpp/tensorrt_llm/runtime/gptSession.cpp
index a871113b..de26f2fd 100644
--- a/cpp/tensorrt_llm/runtime/gptSession.cpp
+++ b/cpp/tensorrt_llm/runtime/gptSession.cpp
@@ -1314,7 +1314,8 @@ void GptSession::CudaGraphExecutor::launch(CudaStream const& stream)
 bool GptSession::CudaGraphExecutor::update(cudaGraph_t const& graph)
 {
     TLLM_LOG_TRACE("%s start", __PRETTY_FUNCTION__);
-    return cudaGraphExecUpdate(mInstance, graph, nullptr) != cudaSuccess;
+    // return cudaGraphExecUpdate(mInstance, graph, nullptr) != cudaSuccess;
+    return false;
 }
 
 void GptSession::CudaGraphExecutor::clear()
diff --git a/cpp/tensorrt_llm/runtime/iBuffer.cpp b/cpp/tensorrt_llm/runtime/iBuffer.cpp
index d3fb05ab..b9bd81bd 100644
--- a/cpp/tensorrt_llm/runtime/iBuffer.cpp
+++ b/cpp/tensorrt_llm/runtime/iBuffer.cpp
@@ -99,7 +99,7 @@ char const* IBuffer::getDataTypeName() const
     case nvinfer1::DataType::kBOOL: return DataTypeTraits<nvinfer1::DataType::kBOOL>::name;
     case nvinfer1::DataType::kUINT8: return DataTypeTraits<nvinfer1::DataType::kUINT8>::name;
     case nvinfer1::DataType::kINT8: return DataTypeTraits<nvinfer1::DataType::kINT8>::name;
-    case nvinfer1::DataType::kFP8: return DataTypeTraits<nvinfer1::DataType::kFP8>::name;
+    // case nvinfer1::DataType::kFP8: return DataTypeTraits<nvinfer1::DataType::kFP8>::name;
     case nvinfer1::DataType::kINT4: /* do nothing */;
     }
     TLLM_THROW("Unknown data type");
diff --git a/cpp/tensorrt_llm/runtime/torchUtils.h b/cpp/tensorrt_llm/runtime/torchUtils.h
index 28c30f23..bf4edab9 100644
--- a/cpp/tensorrt_llm/runtime/torchUtils.h
+++ b/cpp/tensorrt_llm/runtime/torchUtils.h
@@ -89,7 +89,9 @@ public:
         case IBuffer::DataType::kINT32: return torch::kInt32;
         case IBuffer::DataType::kINT64: return torch::kInt64;
         case IBuffer::DataType::kBOOL: return at::ScalarType::Bool;
+#ifdef ENABLE_FP8
         case IBuffer::DataType::kFP8: return at::ScalarType::Bits8;
+#endif
         case IBuffer::DataType::kBF16: return at::ScalarType::BFloat16;
         default: TLLM_THROW("unsupported data type");
         }
@@ -106,7 +108,9 @@ public:
         case torch::kInt32: return IBuffer::DataType::kINT32;
         case torch::kInt64: return IBuffer::DataType::kINT64;
         case at::ScalarType::Bool: return IBuffer::DataType::kBOOL;
+#ifdef ENABLE_FP8
         case at::ScalarType::Bits8: return IBuffer::DataType::kFP8;
+#endif
         case at::ScalarType::BFloat16: return IBuffer::DataType::kBF16;
         default: TLLM_THROW("unsupported data type");
         }
diff --git a/cpp/tensorrt_llm/runtime/utils/debugUtils.cu b/cpp/tensorrt_llm/runtime/utils/debugUtils.cu
index a665a2fa..c35950e2 100644
--- a/cpp/tensorrt_llm/runtime/utils/debugUtils.cu
+++ b/cpp/tensorrt_llm/runtime/utils/debugUtils.cu
@@ -59,8 +59,11 @@ template void invokeCheckTensorNanKernel(float const* data, std::size_t size, in
 template void invokeCheckTensorNanKernel(half const* data, std::size_t size, int* foundNan, cudaStream_t stream);
 template void invokeCheckTensorNanKernel(
     __nv_bfloat16 const* data, std::size_t size, int* foundNan, cudaStream_t stream);
+
+#ifdef ENABLE_FP8
 template void invokeCheckTensorNanKernel(
     __nv_fp8_e4m3 const* data, std::size_t size, int* foundNan, cudaStream_t stream);
+#endif
 
 template <typename T>
 void printLogitsKeyInfo(ITensor const& tensor, std::string const& infoStr)
@@ -126,7 +129,10 @@ void printLogitsKeyInfo(ITensor const& tensor, std::string const& infoStr)
 template void printLogitsKeyInfo<float>(ITensor const& tensor, std::string const& infoStr);
 template void printLogitsKeyInfo<half>(ITensor const& tensor, std::string const& infoStr);
 template void printLogitsKeyInfo<__nv_bfloat16>(ITensor const& tensor, std::string const& infoStr);
+
+#ifdef ENABLE_FP8
 template void printLogitsKeyInfo<__nv_fp8_e4m3>(ITensor const& tensor, std::string const& infoStr);
+#endif
 
 template <typename T>
 bool tensorHasNan(ITensor const& tensor, BufferManager const& manager, std::string const& infoStr)
@@ -145,8 +151,11 @@ template bool tensorHasNan<float>(ITensor const& tensor, BufferManager const& ma
 template bool tensorHasNan<half>(ITensor const& tensor, BufferManager const& manager, std::string const& infoStr);
 template bool tensorHasNan<__nv_bfloat16>(
     ITensor const& tensor, BufferManager const& manager, std::string const& infoStr);
+
+#ifdef ENABLE_FP8
 template bool tensorHasNan<__nv_fp8_e4m3>(
     ITensor const& tensor, BufferManager const& manager, std::string const& infoStr);
+#endif
 
 bool tensorHasNan(
     size_t M, size_t K, nvinfer1::DataType type, void const* data, cudaStream_t stream, std::string const& infoStr)
@@ -166,10 +175,12 @@ bool tensorHasNan(
     {
         return tensorHasNan<__nv_bfloat16>(*tensorView, manager, infoStr);
     }
+#ifdef ENABLE_FP8
     else if (type == nvinfer1::DataType::kFP8)
     {
         return tensorHasNan<__nv_fp8_e4m3>(*tensorView, manager, infoStr);
     }
+#endif
     else
     {
         TLLM_THROW("Not supported type for Nan check");
diff --git a/cpp/tensorrt_llm/thop/fp8Op.cpp b/cpp/tensorrt_llm/thop/fp8Op.cpp
index e13bb78c..7af8e6a0 100644
--- a/cpp/tensorrt_llm/thop/fp8Op.cpp
+++ b/cpp/tensorrt_llm/thop/fp8Op.cpp
@@ -22,7 +22,7 @@
     && ((TORCH_VERSION_MAJOR > 1) || ((TORCH_VERSION_MAJOR == 1) && (TORCH_VERSION_MINOR >= 9)))
 #define TORCH_IS_AT_LEAST_v190
 #endif
-
+#ifdef ENABLE_FP8
 namespace torch_ext
 {
 using torch::Tensor;
@@ -225,3 +225,4 @@ static auto symmetric_dequantize_activation
 
 static auto symmetric_dequantize_per_tensor
     = torch::RegisterOperators("tensorrt_llm::dequantize_e4m3_per_tensor", &torch_ext::symmetric_dequantize_per_tensor);
+#endif // ENABLE_FP8
diff --git a/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp b/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp
index 02156e4e..d70b4dd4 100644
--- a/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp
+++ b/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp
@@ -40,12 +40,14 @@ void check_quant_type_allowed(torch::ScalarType quant_type)
 
 QuantType get_ft_quant_type(torch::ScalarType quant_type, torch::ScalarType activation_type = torch::kFloat16)
 {
+#ifdef ENABLE_FP8
     // Actually we need FP8 here, but current torch version does not support FP8. That's why INT8 is employed here
     if (activation_type == torch::kFloat8_e4m3fn)
     {
         return QuantType::W4_AFP8;
-    }
-    else if (quant_type == torch::kInt8)
+    } else 
+#endif
+    if (quant_type == torch::kInt8)
     {
         return QuantType::W8_A16;
     }
diff --git a/requirements.txt b/requirements.txt
index 2efbdec2..106f7604 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -10,17 +10,17 @@ numpy<2
 onnx>=1.12.0
 polygraphy
 psutil
-pynvml>=11.5.0
+# pynvml>=11.5.0
 pulp
 pandas
 h5py==3.10.0
 StrEnum
 sentencepiece>=0.1.99
-tensorrt~=10.3.0
+# tensorrt~=10.3.0
 # https://github.com/pytorch/pytorch/blob/v2.4.0/version.txt uses 2.4.0a0.
 # https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-07.html#rel-24-07 uses 2.4.0a0.
-torch>=2.4.0a0,<=2.4.0
-nvidia-modelopt~=0.15.0
+# torch>=2.4.0a0,<=2.4.0
+# nvidia-modelopt~=0.15.0
 transformers>=4.38.2,<=4.42.4
 pillow==10.3.0
 wheel
diff --git a/tensorrt_llm/_utils.py b/tensorrt_llm/_utils.py
index c064cd43..02cf889d 100644
--- a/tensorrt_llm/_utils.py
+++ b/tensorrt_llm/_utils.py
@@ -44,29 +44,29 @@ def torch_to_numpy(x: torch.Tensor):
         f'x must be a torch.Tensor object, but got {type(x)}.'
     if x.dtype == torch.bfloat16:
         return x.view(torch.int16).detach().cpu().numpy().view(np_bfloat16)
-    elif x.dtype == torch.float8_e4m3fn:
-        return x.view(torch.int8).detach().cpu().numpy().view(np_float8)
-    else:
-        return x.detach().cpu().numpy()
+    # elif x.dtype == torch.float8_e4m3fn:
+    #     return x.view(torch.int8).detach().cpu().numpy().view(np_float8)
+    # else:
+    return x.detach().cpu().numpy()
 
 
 def numpy_to_torch(x):
     if x.dtype == np_bfloat16:
         return torch.from_numpy(x.view(np.int16)).view(torch.bfloat16)
-    elif x.dtype == np_float8:
-        return torch.from_numpy(x.view(np.int8)).view(torch.float8_e4m3fn)
-    else:
-        return torch.from_numpy(x)
+    # elif x.dtype == np_float8:
+    #     return torch.from_numpy(x.view(np.int8)).view(torch.float8_e4m3fn)
+    # else:
+    return torch.from_numpy(x)
 
 
 def numpy_to_dtype(x, dtype: str):
     if str_dtype_to_np(dtype) == x.dtype:
         return x
-    if x.dtype not in [np_bfloat16, np_float8
-                       ] and dtype not in ['bfloat16', 'fp8']:
-        return x.astype(str_dtype_to_np(dtype))
-    else:
-        return torch_to_numpy(numpy_to_torch(x).to(str_dtype_to_torch(dtype)))
+    # if x.dtype not in [np_bfloat16, np_float8
+    #                    ] and dtype not in ['bfloat16', 'fp8']:
+    #     return x.astype(str_dtype_to_np(dtype))
+    # else:
+    return torch_to_numpy(numpy_to_torch(x).to(str_dtype_to_torch(dtype)))
 
 
 fp32_array = partial(np.array, dtype=np.float32)
@@ -101,10 +101,10 @@ def numpy_array(data, trt_dtype):
 def copy_torch_to_numpy(x: torch.Tensor, ndarray: np.array):
     if x.dtype == torch.bfloat16:
         torch.from_numpy(ndarray.view(np.int16)).copy_(x.view(torch.int16))
-    elif x.dtype == torch.float8_e4m3fn:
-        torch.from_numpy(ndarray.view(np.int8)).copy_(x.view(torch.int8))
-    else:
-        torch.from_numpy(ndarray).copy_(x)
+    # elif x.dtype == torch.float8_e4m3fn:
+    #     torch.from_numpy(ndarray.view(np.int8)).copy_(x.view(torch.int8))
+    # else:
+    torch.from_numpy(ndarray).copy_(x)
     return ndarray
 
 
@@ -132,7 +132,7 @@ _str_to_np_dict = dict(
     int8=np.int8,
     bool=np.bool_,
     bfloat16=np_bfloat16,
-    fp8=np_float8,
+    # fp8=np_float8,
 )
 
 
@@ -150,7 +150,7 @@ _str_to_torch_dtype_dict = dict(
     int32=torch.int32,
     int8=torch.int8,
     bool=torch.bool,
-    fp8=torch.float8_e4m3fn,
+    # fp8=torch.float8_e4m3fn,
 )
 
 
@@ -173,8 +173,8 @@ _str_to_trt_dtype_dict = dict(float16=trt.float16,
                               int32=trt.int32,
                               int8=trt.int8,
                               bool=trt.bool,
-                              bfloat16=trt.bfloat16,
-                              fp8=trt.fp8)
+                              bfloat16=trt.bfloat16,)
+                            #   fp8=trt.fp8)
 
 
 def str_dtype_to_trt(dtype):
@@ -207,7 +207,7 @@ _np_to_trt_dtype_dict = {
     np.dtype('float32'): trt.float32,
     np.dtype('bool'): trt.bool,
     np_bfloat16: trt.bfloat16,
-    np_float8: trt.fp8,
+    # np_float8: trt.fp8,
 }
 
 
@@ -225,7 +225,7 @@ _trt_to_np_dtype_dict = {
     trt.float32: np.float32,
     trt.bool: np.bool_,
     trt.bfloat16: np_bfloat16,
-    trt.fp8: np_float8,
+    # trt.fp8: np_float8,
 }
 
 
@@ -244,7 +244,7 @@ _torch_to_np_dtype_dict = {
     torch.int64: np.int64,
     torch.float16: np.float16,
     torch.bfloat16: np_bfloat16,
-    torch.float8_e4m3fn: np_float8,
+    # torch.float8_e4m3fn: np_float8,
     torch.float32: np.float32,
     torch.float64: np.float64,
     torch.complex64: np.complex64,
@@ -266,7 +266,7 @@ _trt_to_torch_dtype_dict = {
     trt.int8: torch.int8,
     trt.bool: torch.bool,
     trt.bfloat16: torch.bfloat16,
-    trt.fp8: torch.float8_e4m3fn,
+    # trt.fp8: torch.float8_e4m3fn,
 }
 
 
@@ -293,7 +293,7 @@ _torch_to_trt_dtype_dict = {
     torch.int64: trt.int64,
     torch.int32: trt.int32,
     torch.int8: trt.int8,
-    torch.float8_e4m3fn: trt.fp8,
+    # torch.float8_e4m3fn: trt.fp8,
     torch.qint8: trt.int8,
     torch.bool: trt.bool,
     torch.bfloat16: trt.bfloat16
diff --git a/tensorrt_llm/auto_parallel/cluster_info.py b/tensorrt_llm/auto_parallel/cluster_info.py
index 57d63706..f9fce83a 100644
--- a/tensorrt_llm/auto_parallel/cluster_info.py
+++ b/tensorrt_llm/auto_parallel/cluster_info.py
@@ -3,14 +3,18 @@ import re
 from dataclasses import dataclass, field
 from typing import Dict, Tuple, Union
 
-import pynvml
+try:
+    import pynvml
+except ModuleNotFoundError as _:
+    pynvml = None
+
 import torch
 from cuda import cudart
 
 from tensorrt_llm._utils import DictConversion
 from tensorrt_llm.logger import logger
-from tensorrt_llm.profiler import PyNVMLContext, _device_get_memory_info_fn
-
+if pynvml is not None:
+    from tensorrt_llm.profiler import PyNVMLContext, _device_get_memory_info_fn
 
 @dataclass
 class MathThroughput(DictConversion):
@@ -451,79 +455,88 @@ def nvlink_bandwidth(nvlink_version: int) -> int:
 def infer_cluster_info() -> ClusterInfo:
     device = torch.cuda.current_device()
     index = device.index if isinstance(device, torch.device) else device
-    with PyNVMLContext():
-        handle = pynvml.nvmlDeviceGetHandleByIndex(index)
-        compute_cap = pynvml.nvmlDeviceGetCudaComputeCapability(handle)
-        logger.info(f"Compute capability: {compute_cap}")
-        err, properties = cudart.cudaGetDeviceProperties(index)
-        sm_count = properties.multiProcessorCount
-        logger.info(f"SM count: {sm_count}")
-        sm_clock = pynvml.nvmlDeviceGetMaxClockInfo(
-            handle,
-            pynvml.NVML_CLOCK_SM,
-        )
-        logger.info(f"SM clock: {sm_clock} MHz")
-        math_throughput = MathThroughput.to_tflops(
-            ipc_per_sm(compute_cap),
-            sm_count,
-            sm_clock,
-        )
-        for name in math_throughput.__dataclass_fields__:
-            tflops = getattr(math_throughput, name)
-            logger.info(f"{name} TFLOPS: {tflops}")
-
-        mem_info = _device_get_memory_info_fn(handle)
-        memory_budget = mem_info.total // (1024**3)
-        logger.info(f"Total Memory: {memory_budget} GiB")
-
-        mem_clock = pynvml.nvmlDeviceGetMaxClockInfo(
-            handle,
-            pynvml.NVML_CLOCK_MEM,
-        )
-        logger.info(f"Memory clock: {mem_clock} MHz")
-        if pynvml.__version__ < '11.5.0':
-            mem_bus_width = properties.memoryBusWidth
-        else:
-            mem_bus_width = pynvml.nvmlDeviceGetMemoryBusWidth(handle)
-        logger.info(f"Memory bus width: {mem_bus_width}")
-        memory_bw = mem_bus_width * mem_clock * 2 // int(8e3)
-        logger.info(f"Memory bandwidth: {memory_bw} GB/s")
-
-        try:
-            is_nvl_active = bool(pynvml.nvmlDeviceGetNvLinkState(handle, 0))
-            logger.info(f"NVLink is active: {is_nvl_active}")
-        except pynvml.NVMLError:
-            is_nvl_active = False
-
-        intra_node_sharp = False
-        if is_nvl_active:
-            nvl_version_enum = pynvml.nvmlDeviceGetNvLinkVersion(handle, 0)
-            nvl_version = nvlink_version(nvl_version_enum)
-            logger.info(f"NVLink version: {nvl_version}")
-            nvl_bw = nvlink_bandwidth(nvl_version)
-            logger.info(f"NVLink bandwidth: {nvl_bw} GB/s")
-            intra_node_bw = nvl_bw
-            if nvl_version >= 4:
-                intra_node_sharp = True
-        else:
+    if pynvml is not None:
+        with PyNVMLContext():
+            handle = pynvml.nvmlDeviceGetHandleByIndex(index)
+            compute_cap = pynvml.nvmlDeviceGetCudaComputeCapability(handle)
+            logger.info(f"Compute capability: {compute_cap}")
+            err, properties = cudart.cudaGetDeviceProperties(index)
+            sm_count = properties.multiProcessorCount
+            logger.info(f"SM count: {sm_count}")
+            sm_clock = pynvml.nvmlDeviceGetMaxClockInfo(
+                handle,
+                pynvml.NVML_CLOCK_SM,
+            )
+            logger.info(f"SM clock: {sm_clock} MHz")
+            math_throughput = MathThroughput.to_tflops(
+                ipc_per_sm(compute_cap),
+                sm_count,
+                sm_clock,
+            )
+            for name in math_throughput.__dataclass_fields__:
+                tflops = getattr(math_throughput, name)
+                logger.info(f"{name} TFLOPS: {tflops}")
+
+            mem_info = _device_get_memory_info_fn(handle)
+            memory_budget = mem_info.total // (1024**3)
+            logger.info(f"Total Memory: {memory_budget} GiB")
+
+            mem_clock = pynvml.nvmlDeviceGetMaxClockInfo(
+                handle,
+                pynvml.NVML_CLOCK_MEM,
+            )
+            logger.info(f"Memory clock: {mem_clock} MHz")
             if pynvml.__version__ < '11.5.0':
-                pcie_gen = pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle)
-                pcie_speed = (2**pcie_gen) * 1000
+                mem_bus_width = properties.memoryBusWidth
             else:
-                pcie_speed = pynvml.nvmlDeviceGetPcieSpeed(handle)
-            logger.info(f"PCIe speed: {pcie_speed} Mbps")
-            pcie_link_width = pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle)
-            logger.info(f"PCIe link width: {pcie_link_width}")
-            pcie_bw = pcie_speed * pcie_link_width // int(8e3)
-            logger.info(f"PCIe bandwidth: {pcie_bw} GB/s")
-            intra_node_bw = pcie_bw
-
+                mem_bus_width = pynvml.nvmlDeviceGetMemoryBusWidth(handle)
+            logger.info(f"Memory bus width: {mem_bus_width}")
+            memory_bw = mem_bus_width * mem_clock * 2 // int(8e3)
+            logger.info(f"Memory bandwidth: {memory_bw} GB/s")
+
+            try:
+                is_nvl_active = bool(pynvml.nvmlDeviceGetNvLinkState(handle, 0))
+                logger.info(f"NVLink is active: {is_nvl_active}")
+            except pynvml.NVMLError:
+                is_nvl_active = False
+
+            intra_node_sharp = False
+            if is_nvl_active:
+                nvl_version_enum = pynvml.nvmlDeviceGetNvLinkVersion(handle, 0)
+                nvl_version = nvlink_version(nvl_version_enum)
+                logger.info(f"NVLink version: {nvl_version}")
+                nvl_bw = nvlink_bandwidth(nvl_version)
+                logger.info(f"NVLink bandwidth: {nvl_bw} GB/s")
+                intra_node_bw = nvl_bw
+                if nvl_version >= 4:
+                    intra_node_sharp = True
+            else:
+                if pynvml.__version__ < '11.5.0':
+                    pcie_gen = pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle)
+                    pcie_speed = (2**pcie_gen) * 1000
+                else:
+                    pcie_speed = pynvml.nvmlDeviceGetPcieSpeed(handle)
+                logger.info(f"PCIe speed: {pcie_speed} Mbps")
+                pcie_link_width = pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle)
+                logger.info(f"PCIe link width: {pcie_link_width}")
+                pcie_bw = pcie_speed * pcie_link_width // int(8e3)
+                logger.info(f"PCIe bandwidth: {pcie_bw} GB/s")
+                intra_node_bw = pcie_bw
+
+            cluster_info = ClusterInfo(
+                math_throughput=math_throughput,
+                memory_bw=memory_bw,
+                memory_budget_per_device=memory_budget,
+                intra_node_bw_per_device=intra_node_bw,
+                intra_node_sharp=intra_node_sharp,
+            )
+    else:
         cluster_info = ClusterInfo(
-            math_throughput=math_throughput,
-            memory_bw=memory_bw,
-            memory_budget_per_device=memory_budget,
-            intra_node_bw_per_device=intra_node_bw,
-            intra_node_sharp=intra_node_sharp,
+            math_throughput=-1,
+            memory_bw=-1,
+            memory_budget_per_device=-1,
+            intra_node_bw_per_device=-1,
+            intra_node_sharp=-1,
         )
     return cluster_info
 
@@ -533,6 +546,10 @@ def infer_cluster_config() -> Dict[str, Union[str, ClusterInfo]]:
     cluster_key = infer_cluster_key()
     if cluster_key is not None:
         return dict(cluster_key=cluster_key)
+    elif pynvml is None:
+        cluster_info = infer_cluster_info()
+        return dict(cluster_key=device_name.replace(" ", "-"),
+                    cluster_info=cluster_info)
     else:
         try:
             cluster_info = infer_cluster_info()
diff --git a/tensorrt_llm/builder.py b/tensorrt_llm/builder.py
index 95559465..1d9e4829 100644
--- a/tensorrt_llm/builder.py
+++ b/tensorrt_llm/builder.py
@@ -683,7 +683,7 @@ class Engine:
         self,
         config: EngineConfig,
         engine: Union[trt.IHostMemory, None],
-        managed_weights: list[tuple[str, np.ndarray]] = None,
+        managed_weights = None, # : list[tuple[str, np.ndarray]] = None,
     ):
         self.config = config
         self.engine = engine
@@ -879,8 +879,8 @@ def _init_max_seq_len(model_config, build_config):
             assert build_config.max_input_len <= build_config.max_seq_len, 'max_input_len should not be larger than max_seq_len'
 
 
-def serialize_managed_weights(managed_weights: list[tuple[str, np.ndarray]],
-                              path: str | Path,
+def serialize_managed_weights(managed_weights, # : list[tuple[str, np.ndarray]],
+                              path, # : str | Path,
                               metadata=None) -> None:
     header = {}
     if metadata is not None:
@@ -924,7 +924,7 @@ def serialize_managed_weights(managed_weights: list[tuple[str, np.ndarray]],
 
 def build(model: PretrainedModel,
           build_config: BuildConfig,
-          return_build_config: bool = False) -> Engine | BuildConfig:
+          return_build_config: bool = False): # -> Engine | BuildConfig:
     '''Build engine from given model and optimization options specified in the build_config
        WARNING: this function may change the given \p model object state in some optimization passes
        to avoid cloning a model since normally the LLM models consumes large memory.
diff --git a/tensorrt_llm/executor.py b/tensorrt_llm/executor.py
index 02167ad3..97b2ca91 100644
--- a/tensorrt_llm/executor.py
+++ b/tensorrt_llm/executor.py
@@ -97,7 +97,7 @@ class GenerationRequest:
         return request
 
 
-@dataclass(slots=True)
+@dataclass # (slots=True)
 class CompletionOutput:
     """The output data of one completion output of a request.
 
diff --git a/tensorrt_llm/hlapi/build_cache.py b/tensorrt_llm/hlapi/build_cache.py
index 586bf17c..491c0c9e 100644
--- a/tensorrt_llm/hlapi/build_cache.py
+++ b/tensorrt_llm/hlapi/build_cache.py
@@ -16,7 +16,7 @@ from tensorrt_llm.hlapi.llm_utils import BuildConfig
 from tensorrt_llm.logger import logger
 
 
-def get_build_cache_config_from_env() -> tuple[bool, str]:
+def get_build_cache_config_from_env(): # -> tuple[bool, str]:
     """
     Get the build cache configuration from the environment variables
     """
diff --git a/tensorrt_llm/hlapi/llm_utils.py b/tensorrt_llm/hlapi/llm_utils.py
index 0315b43e..57b5d760 100644
--- a/tensorrt_llm/hlapi/llm_utils.py
+++ b/tensorrt_llm/hlapi/llm_utils.py
@@ -763,8 +763,8 @@ class ModelLoader:
 
     def __init__(self,
                  llm_args: LlmArgs,
-                 workspace: Optional[str | tempfile.TemporaryDirectory] = None,
-                 llm_build_stats: Optional["LlmBuildStats"] = None):
+                 workspace = None, # : Optional[str | tempfile.TemporaryDirectory] = None,
+                 llm_build_stats = None, ): # : Optional["LlmBuildStats"] = None):
         self.llm_args = llm_args
         self._workspace = workspace or tempfile.TemporaryDirectory()
         self.llm_build_stats = llm_build_stats or LlmBuildStats()
@@ -1163,8 +1163,8 @@ class CachedModelLoader:
         self,
         llm_args: LlmArgs,
         llm_build_stats: "LlmBuildStats",
-        mpi_session: Optional[MpiSession] = None,
-        workspace: Optional[str] = None,
+        mpi_session = None, # : Optional[MpiSession] = None,
+        workspace = None, # : Optional[str] = None,
     ):
         self.llm_args = llm_args
         self.mpi_session = mpi_session
@@ -1328,9 +1328,9 @@ class CachedModelLoader:
     @staticmethod
     def _node_build_task(
         llm_args: LlmArgs,
-        workspace: Optional[str | tempfile.TemporaryDirectory] = None,
-        llm_build_stats: Optional['LlmBuildStats'] = None,
-        engine_dir: Optional[Path] = None,
+        workspace, # : Optional[str | tempfile.TemporaryDirectory] = None,
+        llm_build_stats, # : Optional['LlmBuildStats'] = None,
+        engine_dir, # : Optional[Path] = None,
     ):
         if MPINodeState.is_initialized():
             raise RuntimeError("The MPI node is already initialized.")
diff --git a/tensorrt_llm/hlapi/utils.py b/tensorrt_llm/hlapi/utils.py
index 6aaf2811..4df35eef 100644
--- a/tensorrt_llm/hlapi/utils.py
+++ b/tensorrt_llm/hlapi/utils.py
@@ -32,7 +32,7 @@ def print_traceback_on_error(func):
     return wrapper
 
 
-@dataclass(slots=True)
+@dataclass # (slots=True)
 class SamplingParams:
     """
     Sampling parameters for text generation.
@@ -88,15 +88,15 @@ class SamplingParams:
     pad_id: Optional[int] = None
     max_new_tokens: int = 32
 
-    bad: Optional[Union[str, List[str]]] = None
-    bad_token_ids: Optional[List[int]] = None
-    _bad_word_ids: Optional[List[List[int]]] = field(default=None,
+    bad = None # : Optional[Union[str, List[str]]] = None
+    bad_token_ids = None # : Optional[List[int]] = None
+    _bad_word_ids: Optional[List] = field(default=None,
                                                      init=False,
                                                      repr=False)
-    stop: Optional[Union[str, List[str]]] = None
-    stop_token_ids: Optional[List[int]] = None
+    stop = None # : Optional[Union[str, List[str]]] = None
+    stop_token_ids = None # : Optional[List[int]] = None
     include_stop_str_in_output: bool = False
-    _stop_word_ids: Optional[List[List[int]]] = field(default=None,
+    _stop_word_ids: Optional[List] = field(default=None,
                                                       init=False,
                                                       repr=False)
 
diff --git a/tensorrt_llm/models/deci/layer_config.py b/tensorrt_llm/models/deci/layer_config.py
index 84ed3487..e68868f9 100644
--- a/tensorrt_llm/models/deci/layer_config.py
+++ b/tensorrt_llm/models/deci/layer_config.py
@@ -30,7 +30,7 @@ class FFNImplementation(str, enum.Enum):
     NO_OP = "no_op"
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass # (frozen=True, kw_only=True)
 class AttentionConfig:
     impl: AttentionImplementation = AttentionImplementation.ATTENTION
     num_key_value_heads: Optional[int] = None
@@ -40,13 +40,13 @@ class AttentionConfig:
         return self.impl == AttentionImplementation.ATTENTION
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass # (frozen=True, kw_only=True)
 class FFNConfig:
     impl: FFNImplementation = FFNImplementation.MLP
     intermediate_size: Optional[int] = None
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass # (frozen=True, kw_only=True)
 class DeciLayerConfig:
     attention: AttentionConfig = field(default_factory=AttentionConfig)
     ffn: FFNConfig = field(default_factory=FFNConfig)
diff --git a/tensorrt_llm/models/gemma/convert.py b/tensorrt_llm/models/gemma/convert.py
index 4a4a3642..8d7e0225 100644
--- a/tensorrt_llm/models/gemma/convert.py
+++ b/tensorrt_llm/models/gemma/convert.py
@@ -809,7 +809,7 @@ def load_gemma_weights(
     return weights
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass # (frozen=True, kw_only=True)
 class QuantizeModifiers:
     """
     Bag of additional conversion parameters, sourced from argparse or from defaults.
diff --git a/tensorrt_llm/models/gemma/utils/params.py b/tensorrt_llm/models/gemma/utils/params.py
index f0445f91..a0aa54de 100644
--- a/tensorrt_llm/models/gemma/utils/params.py
+++ b/tensorrt_llm/models/gemma/utils/params.py
@@ -22,10 +22,10 @@ open sourcing.
 import functools
 from typing import Any
 
-Params = dict[str, Any]
+Params = dict # [str, Any]
 
 
-@functools.cache
+# @functools.cache
 def load_params(path: str) -> Params:
     import orbax.checkpoint
     """Loads parameters from a checkpoint path."""
diff --git a/tensorrt_llm/models/llama/config.py b/tensorrt_llm/models/llama/config.py
index 912ab4dd..8720f1f1 100644
--- a/tensorrt_llm/models/llama/config.py
+++ b/tensorrt_llm/models/llama/config.py
@@ -109,10 +109,12 @@ class LLaMAConfig(PretrainedConfig):
                 hf_config = LlavaNextConfig.from_pretrained(
                     hf_config_dir).text_config
             if hf_config.model_type == "llava_llama":
-                hf_config.llm_cfg["architecture"] = hf_config.llm_cfg[
-                    "architectures"][0]
-                hf_config.llm_cfg["dtype"] = hf_config.llm_cfg["torch_dtype"]
-                hf_config = PretrainedConfig.from_dict(hf_config.llm_cfg)
+                if hasattr(hf_config, "llm_cfg"):
+                    # a specific patch for omnidrive
+                    hf_config.llm_cfg["architecture"] = hf_config.llm_cfg[
+                        "architectures"][0]
+                    hf_config.llm_cfg["dtype"] = hf_config.llm_cfg["torch_dtype"]
+                    hf_config = PretrainedConfig.from_dict(hf_config.llm_cfg)
 
         num_key_value_heads = getattr(hf_config, "num_key_value_heads",
                                       hf_config.num_attention_heads)
diff --git a/tensorrt_llm/models/model_weights_loader.py b/tensorrt_llm/models/model_weights_loader.py
index 3b55f2e3..8d0fc22e 100644
--- a/tensorrt_llm/models/model_weights_loader.py
+++ b/tensorrt_llm/models/model_weights_loader.py
@@ -65,7 +65,7 @@ class ModelWeightsLoader:
 
     def translate_to_external_key(
             self, tllm_key: str,
-            tllm_to_externel_key_dict: dict) -> str | List[str]:
+            tllm_to_externel_key_dict: dict): # -> str | List[str]:
         """Translate TRT-LLM key into HF key or HF key list (e.g. QKV/MoE/GPTQ)
 
         tllm_key will get translated into HF format section by section.
diff --git a/tensorrt_llm/models/modeling_utils.py b/tensorrt_llm/models/modeling_utils.py
index fc7577ff..8b6f2b11 100644
--- a/tensorrt_llm/models/modeling_utils.py
+++ b/tensorrt_llm/models/modeling_utils.py
@@ -42,7 +42,7 @@ from .convert_utils import weight_only_quantize_dict
 from .generation_mixin import GenerationMixin
 
 
-@dataclasses.dataclass(kw_only=True, frozen=True)
+@dataclasses.dataclass # (kw_only=True, frozen=True)
 class Gemma2ConfigGroup:
     query_pre_attn_scalar: int
     final_logit_softcapping: Optional[float]
diff --git a/tensorrt_llm/parameter.py b/tensorrt_llm/parameter.py
index f405387a..2d062983 100644
--- a/tensorrt_llm/parameter.py
+++ b/tensorrt_llm/parameter.py
@@ -222,7 +222,7 @@ class Parameter:
             return network.trt_network.set_weights_name(
                 self._get_weights(network), name)
 
-    def _get_weights(self, network) -> trt.Weights | Tensor | None:
+    def _get_weights(self, network): # -> trt.Weights | Tensor | None:
         tensor = network.get_parameter_tensor(self)
         if self.is_managed(network):
             return tensor
diff --git a/tensorrt_llm/plugin/plugin.py b/tensorrt_llm/plugin/plugin.py
index 5b24dd85..f83e7656 100644
--- a/tensorrt_llm/plugin/plugin.py
+++ b/tensorrt_llm/plugin/plugin.py
@@ -114,7 +114,7 @@ class PluginConfigMeta(type):
         return super().__new__(cls, name, bases, attrs)
 
 
-@dataclass(slots=True)
+@dataclass # (slots=True)
 class PluginConfig(metaclass=PluginConfigMeta):
     """The config that manages plugin-related options.
 
diff --git a/tensorrt_llm/runtime/generation.py b/tensorrt_llm/runtime/generation.py
index 9e8c46c7..6e356ba3 100755
--- a/tensorrt_llm/runtime/generation.py
+++ b/tensorrt_llm/runtime/generation.py
@@ -1534,7 +1534,7 @@ class GenerationSession(object):
               encoder_max_input_length: Optional[int] = None,
               lora_manager: LoraManager = None,
               lora_uids: List[str] = None,
-              medusa_choices: List[List[int]] = None,
+              medusa_choices: List = None,
               multi_block_mode: bool = None,
               enable_context_fmha_fp32_acc: bool = None):
         # Store these params related to buffer size to check against
diff --git a/tensorrt_llm/runtime/model_runner_cpp.py b/tensorrt_llm/runtime/model_runner_cpp.py
index 472d5994..e0e1ce61 100644
--- a/tensorrt_llm/runtime/model_runner_cpp.py
+++ b/tensorrt_llm/runtime/model_runner_cpp.py
@@ -75,20 +75,20 @@ class ModelRunnerCpp(ModelRunnerMixin):
         max_input_len: Optional[int] = None,
         max_output_len: Optional[int] = None,
         max_beam_width: Optional[int] = None,
-        max_attention_window_size: Optional[list[int]] = None,
+        max_attention_window_size = None, # : Optional[list[int]] = None,
         sink_token_length: Optional[int] = None,
         kv_cache_free_gpu_memory_fraction: Optional[float] = None,
-        medusa_choices: list[list[int]] | None = None,
-        lookahead_config: list[int] | None = None,
+        medusa_choices = None, # : list[list[int]] | None = None,
+        lookahead_config = None, # : list[int] | None = None,
         debug_mode: bool = False,
         lora_ckpt_source: str = "hf",
         gpu_weights_percent: float = 1,
-        max_tokens_in_paged_kv_cache: int | None = None,
+        max_tokens_in_paged_kv_cache = None, # : int | None = None,
         kv_cache_enable_block_reuse: bool = False,
         enable_chunked_context: bool = False,
         is_enc_dec: bool = False,
-        multi_block_mode: Optional[bool] = None,
-        enable_context_fmha_fp32_acc: Optional[bool] = None
+        multi_block_mode = None, # : Optional[bool] = None,
+        enable_context_fmha_fp32_acc = None, # : Optional[bool] = None
     ) -> 'ModelRunnerCpp':
         """
         Create a ModelRunnerCpp instance from an engine directory.
@@ -356,30 +356,29 @@ class ModelRunnerCpp(ModelRunnerMixin):
 
     def generate(
             self,
-            batch_input_ids: List[torch.Tensor],
+            batch_input_ids, # : List[torch.Tensor],
             *,
-            encoder_input_ids: List[torch.Tensor] = None,
-            encoder_input_features: List[
-                torch.Tensor] = None,  # TODO: add to doc string
-            encoder_output_lengths: List[int] = None,
-            sampling_config: Optional[SamplingConfig] = None,
-            lora_uids: Optional[list] = None,
+            encoder_input_ids = None, # : List[torch.Tensor] = None,
+            encoder_input_features = None, # : List[torch.Tensor] = None,  # TODO: add to doc string
+            encoder_output_lengths = None, # : List[int] = None,
+            sampling_config = None, # : Optional[SamplingConfig] = None,
+            lora_uids = None, # : Optional[list] = None,
             streaming: bool = False,
-            stopping_criteria: Optional[StoppingCriteria] = None,
-            logits_processor: Optional[LogitsProcessor] = None,
+            stopping_criteria = None, # : Optional[StoppingCriteria] = None,
+            logits_processor = None, # : Optional[LogitsProcessor] = None,
             max_new_tokens: int = 1,
-            end_id: int | None = None,
-            pad_id: int | None = None,
-            bad_words_list: list[list[int]] | None = None,
-            stop_words_list: list[list[int]] | None = None,
+            end_id = None, # : int | None = None,
+            pad_id = None, # : int | None = None,
+            bad_words_list = None, # : list[list[int]] | None = None,
+            stop_words_list = None, # : list[list[int]] | None = None,
             return_dict: bool = False,
             output_sequence_lengths: bool = False,
             output_log_probs: bool = False,
             output_cum_log_probs: bool = False,
-            prompt_table: Optional[Union[str, torch.Tensor]] = None,
-            prompt_tasks: Optional[str] = None,
+            prompt_table = None, # : Optional[Union[str, torch.Tensor]] = None,
+            prompt_tasks = None, # : Optional[str] = None,
             return_all_generated_tokens: bool = False,
-            **kwargs) -> Union[torch.Tensor, dict]:
+            **kwargs): # -> Union[torch.Tensor, dict]:
         """
         Generates sequences of token ids.
         The generation-controlling parameters are set in the sampling_config; it will be set to a default one if not passed.
